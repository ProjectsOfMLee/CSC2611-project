{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from data_preprocess import *\n",
    "import pickle\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "#augment_data = pickle.load(open(\"extra_s2orc.pickle\",\"rb\"))\n",
    "#for yr in range(1994,2022):\n",
    "    #Data_Augmentation(year=yr)\n",
    "\n",
    "def Build_mapping(year:str):\n",
    "    '''\n",
    "    Build index to word and word to index map for given year\n",
    "    If year is not available, build map for entire corpus\n",
    "    '''\n",
    "    target_files = [x for x in os.listdir(data_path)]\n",
    "    index2word={}\n",
    "    word2index={}\n",
    "    if year == \"all\":\n",
    "        for f in target_files:\n",
    "            temp = open(data_path+f,\"r\",encoding='utf-8').read()\n",
    "            for word in temp.split():\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "                    index2word[len(word2index)-1] = word\n",
    "    else:\n",
    "        temp = open(data_path+f\"{year}.txt\",\"r\",encoding='utf-8').read()\n",
    "        for word in temp.split():\n",
    "            if word not in word2index:\n",
    "                word2index[word] = len(word2index)\n",
    "                index2word[len(word2index)-1] = word\n",
    "\n",
    "    return index2word, word2index\n",
    "\n",
    "def Build_freq_dict(year:str):\n",
    "    '''\n",
    "    Build frequency dictionary for given year\n",
    "    '''\n",
    "    f = open(data_path+f\"{year}.txt\",\"r\",encoding='utf-8').read()\n",
    "    unigram = FreqDist(f.split())\n",
    "    return unigram\n",
    "\n",
    "\n",
    "def Get_Unigram(word:str, year:str):\n",
    "    '''\n",
    "    return frequency of a word in text from a given year\n",
    "    '''\n",
    "    f = open(data_path+f\"{year}.txt\",\"r\",encoding='utf-8').read()\n",
    "    unigram = FreqDist(f.split())\n",
    "    return unigram[word] if word in unigram else 0\n",
    "\n",
    "def Get_topk(k:int, year:str):\n",
    "    '''\n",
    "    Get top k common words in the text from given year\n",
    "    '''\n",
    "    f = open(data_path+f\"{year}.txt\",\"r\",encoding='utf-8').read()\n",
    "    freq = FreqDist(f.split())\n",
    "    return freq.most_common(k)\n",
    "\n",
    "def Get_sentences(year:str):\n",
    "    '''\n",
    "    get sentences from unprocessed data for a given year\n",
    "    '''\n",
    "    data = json.load(open(data_path+'unprocessed.json'))\n",
    "    abstract = \"\"\n",
    "    for record in data[year]:\n",
    "        abstract += record[\"abstract\"]\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].replace(\"\\n\",\" \")\n",
    "        temp = remove_punctuation(sentences[i])\n",
    "        temp = to_lower_case(temp)\n",
    "        temp = remove_stopwords(temp.split())\n",
    "        temp = lemmatise_verbs(temp)\n",
    "        temp = remove_numbers(temp)\n",
    "        sentences[i] = \" \".join(temp)\n",
    "        \n",
    "    for s in sentences:    \n",
    "        if (not s) or (len(s.split())<2):\n",
    "            sentences.remove(s)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def Data_Augmentation(year:int):\n",
    "\n",
    "    file1 = open(f\"./data/{year}.txt\")\n",
    "    lines = file1.read()\n",
    "    extra = \" \".join(augment_data[year])\n",
    "    extra = remove_punctuation(extra)\n",
    "    extra = to_lower_case(extra)\n",
    "    extra = remove_stopwords(extra)\n",
    "    extra = lemmatise_verbs(extra)\n",
    "    extra = remove_numbers(extra)\n",
    "    #lines += \" \"\n",
    "    lines += (\"\".join(extra))\n",
    "    out_file = open(f\"./data_augmented/{year}.txt\", \"w\",encoding='utf-8')\n",
    "    out_file.write(lines)\n",
    "    \n",
    "    file1.close()\n",
    "    out_file.close()\n",
    "\n",
    "def Get_common_words(threshold):\n",
    "\n",
    "    '''\n",
    "    Get common words across all years and filter the words that have frequency more than threhold\n",
    "    '''\n",
    "    common_words = set([k for k, v in Build_freq_dict(year=\"1994\").items() if v > threshold])\n",
    "    for yr in trange(1995,2021):\n",
    "        \n",
    "        temp = [k for k, v in Build_freq_dict(year=f\"{yr}\").items() if v > threshold]\n",
    "        common_words.intersection_update(temp)\n",
    "    \n",
    "    common_words = sorted(common_words)\n",
    "    return common_words\n",
    "\n",
    "def co_occurrence(sentences, window_size):\n",
    "    '''\n",
    "    Get co-occurence of words from string of sentences within the window_size\n",
    "    '''\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "\n",
    "    text = sentences.split()\n",
    "    for i in trange(len(text)):\n",
    "        token = text[i]\n",
    "        vocab.add(token)  \n",
    "        next_token = text[i+1 : i+1+window_size]\n",
    "        for t in next_token:\n",
    "            key = tuple(sorted([t, token]))\n",
    "            d[key] += 1\n",
    "            \n",
    "    vocab = sorted(vocab)  \n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    \n",
    "    df=df[list(common_words)]\n",
    "    df=df.loc[df.index.isin(common_words)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def construct_ppmi(df):\n",
    "    '''\n",
    "    construct ppmi based on co-occurence dataframe\n",
    "    '''\n",
    "    col_totals = df.sum(axis=0)\n",
    "    total = col_totals.sum()\n",
    "    row_totals = df.sum(axis=1)\n",
    "    expected = np.outer(row_totals, col_totals) / total\n",
    "    df = df / expected\n",
    "    with np.errstate(divide='ignore'):\n",
    "        df = np.log(df)\n",
    "    df[np.isinf(df)] = 0.0  \n",
    "    df[df < 0] = 0.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_ppmi_embedding(year,window_size):\n",
    "    '''\n",
    "    get ppmi embedding for given word for the text in given year\n",
    "    '''\n",
    "    text = open(data_path+f\"{year}.txt\",\"r\",encoding='utf-8').read()\n",
    "    df_cooccurrence = co_occurrence(text, window_size=4)\n",
    "\n",
    "    #from IPython.display import display, HTML\n",
    "    #display(HTML(df_cooccurrence.to_html()))\n",
    "\n",
    "    df_ppmi=construct_ppmi(df_cooccurrence).sort_index(ascending=True)\n",
    "    df_ppmi = df_ppmi.reindex(sorted(df_ppmi.columns), axis=1)\n",
    "    return df_ppmi\n",
    "\n",
    "def get_nearest_neighbor(word,ppmi_matrix,k):\n",
    "    # This is a helper function which gets k nearest neighbor words\n",
    "    if (word not in common_words):\n",
    "        return []\n",
    "    \n",
    "    matrix_year = []\n",
    "    for wd in common_words:\n",
    "        matrix_year.append(list(ppmi_matrix[wd]))\n",
    "    matrix_year = np.array(matrix_year)\n",
    "\n",
    "    knn_temp = NearestNeighbors(n_neighbors=k)\n",
    "    knn_temp.fit(matrix_year)\n",
    "\n",
    "    vector_temp = np.asarray(list(ppmi_matrix[word])).reshape(1, -1)\n",
    "    neighbor_idx = knn_temp.kneighbors(vector_temp,k,return_distance=False)\n",
    "    #print(neighbor_idx)\n",
    "    \n",
    "    ret = [common_words[i] for i in list(neighbor_idx)[0]]\n",
    "    ret.remove(word)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "Unique_Tokens=[]\n",
    "list_yr=[]\n",
    "for yr in trange(1994,2022):\n",
    "    list_yr.append(yr)\n",
    "    Unique_Tokens.append(len(Build_mapping(year=yr)[1]))\n",
    "df_result=pd.DataFrame({\"Year\":list_yr,\"Unique Tokens\":Unique_Tokens})\n",
    "df_result.set_index(\"Year\",inplace=True)\n",
    "df_result.plot(title=\"Unique Tokens\",grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [06:22<00:00, 14.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6501"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./data_augmented/\"\n",
    "common_words=Get_common_words(threshold=20)\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3173119/3173119 [00:22<00:00, 142058.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#ppmi_1994=get_ppmi_embedding(year=1994,window_size=4)\n",
    "ppmi_2020=get_ppmi_embedding(year=2020,window_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_nearest_neighbor(word=\"deep\",ppmi_matrix=ppmi_1994,k=6)\n",
    "#get_nearest_neighbor(word=\"deep\",ppmi_matrix=ppmi_2020,k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'pre',\n",
       " 'use',\n",
       " 'learn',\n",
       " 'propose',\n",
       " 'task',\n",
       " 'base',\n",
       " 'language',\n",
       " 'tune',\n",
       " 'approach',\n",
       " 'result',\n",
       " 'fine',\n",
       " 'neural',\n",
       " 'art',\n",
       " 'performance',\n",
       " 'outperform',\n",
       " 'multi',\n",
       " 'paper',\n",
       " 'improve']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_nearest_neighbor(word=\"corpora\",ppmi_matrix=ppmi_1994,k=6)\n",
    "get_nearest_neighbor(word=\"train\",ppmi_matrix=ppmi_2020,k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most and least changed context words\n",
    "from scipy.spatial import distance\n",
    "semantic_change = []\n",
    "for word in common_words:\n",
    "    embed_a = list(ppmi_1994[word])\n",
    "    embed_b = list(ppmi_2020[word])\n",
    "    cos_simi = distance.cosine(embed_a,embed_b)\n",
    "    semantic_change.append(cos_simi)\n",
    "[x for _,x in sorted(zip(semantic_change,common_words),reverse=False)][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.cosine(list(ppmi_1994[\"characterizations\"]),list(ppmi_2020[\"characterizations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GB before Loading the Model: 0.13\n",
      "----------\n",
      "31.84 seconds taken to load\n",
      "Finished loading Word2Vec\n",
      "Memory used in GB after Loading the Model: 4.26\n",
      "Percentage increase in memory usage: 3184.54% \n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import pickle\n",
    "import os\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "from psutil import virtual_memory\n",
    "import time #This module is used to calculate the time  \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# The following code is good for detect model memory usage and tiem to load  \n",
    "process = psutil.Process(os.getpid())\n",
    "mem = virtual_memory()\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "#load the model\n",
    "if os.path.isfile('nlp_w2v_googlenews300.pkl'):\n",
    "    g300_vectors = pickle.load(open(\"nlp_w2v_googlenews300.pkl\",\"rb\"))\n",
    "else:\n",
    "    g300_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "    pickle.dump(g300_vectors, open(\"nlp_w2v_googlenews300.pkl\",\"wb\"))\n",
    "\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('Finished loading Word2Vec')\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print(\"Numver of words in vocablulary: \",len(g300_vectors.key_to_index.keys())) #Number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GB before Loading the Model: 4.06\n",
      "----------\n",
      "2.48 seconds taken to load\n",
      "Finished loading Word2Vec\n",
      "Memory used in GB after Loading the Model: 4.06\n",
      "Percentage increase in memory usage: 100.14% \n",
      "Numver of words in vocablulary:  69592\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "# The following code is good for detect model memory usage and tiem to load  \n",
    "process = psutil.Process(os.getpid())\n",
    "mem = virtual_memory()\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "#load the model\n",
    "\n",
    "trained_cs_vectors = Word2Vec.load(\"./model/2020.model\")\n",
    "\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('Finished loading Word2Vec')\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print(\"Numver of words in vocablulary: \",len(trained_cs_vectors.wv.key_to_index.keys())) #Number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trains', 0.8081232309341431), ('Train', 0.6729269623756409), ('commuter_train', 0.6523401737213135), ('locomotive', 0.6395583152770996), ('freight_train', 0.6207071542739868), ('railway', 0.6071821451187134), ('bus', 0.6067740321159363), ('Trains', 0.5963783264160156), ('rail', 0.5885170102119446), ('commuter_trains', 0.5821391940116882)]\n",
      "['model', 'pre', 'use', 'learn', 'propose', 'task', 'base', 'language', 'tune']\n",
      "[('initialize', 0.8524168729782104), ('pre', 0.8446813821792603), ('substitute', 0.8370950818061829), ('pseudo', 0.8242562413215637), ('unpaired', 0.8195383548736572), ('tune', 0.8185880184173584), ('scratch', 0.8175374269485474), ('freeze', 0.8033123016357422), ('pretraining', 0.8025900721549988), ('compress', 0.7985637187957764)]\n"
     ]
    }
   ],
   "source": [
    "WORD=\"train\"\n",
    "print(g300_vectors.most_similar(WORD, topn=10))\n",
    "print(get_nearest_neighbor(word=WORD,ppmi_matrix=ppmi_2020,k=10))\n",
    "print(trained_cs_vectors.wv.most_similar(WORD, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 0.7262316942214966), ('datasets', 0.603030264377594), ('dataset', 0.5796632170677185), ('databases', 0.5450120568275452), ('statistics', 0.537885844707489), ('information', 0.5368290543556213), ('database', 0.5325667262077332), ('Data_System_IPEDS', 0.5222618579864502), ('data.The', 0.5189104080200195), ('OpenSpirit_enabled', 0.5174090266227722)]\n",
      "['model', 'paper', 'information', 'propose', 'approach', 'methods', 'use', 'proposed', 'performance']\n",
      "[('stored', 0.8254101276397705), ('storing', 0.794579803943634), ('gathering', 0.7672482132911682), ('amounts', 0.7385837435722351), ('collection', 0.729941725730896), ('records', 0.7275473475456238), ('collecting', 0.7199752926826477), ('big', 0.7184199094772339), ('mined', 0.7163366079330444), ('streams', 0.7047645449638367)]\n"
     ]
    }
   ],
   "source": [
    "WORD=\"data\"\n",
    "print(g300_vectors.most_similar(WORD, topn=10))\n",
    "print(get_nearest_neighbor(word=WORD,ppmi_matrix=ppmi_2020,k=10))\n",
    "print(trained_cs_vectors.wv.most_similar(WORD, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('langauge', 0.7476695775985718), ('Language', 0.6695356369018555), ('languages', 0.6341332197189331), ('English', 0.6120712757110596), ('CMPB_Spanish', 0.6083105206489563), ('nonnative_speakers', 0.6063110828399658), ('idiomatic_expressions', 0.5889802575111389), ('verb_tenses', 0.5841569304466248), ('Kumeyaay_Diegueno', 0.5798824429512024), ('dialect', 0.5724600553512573), ('Amharic_Ethiopia', 0.57015061378479), ('vernacular_Leo', 0.5687463879585266), ('Euskera', 0.5647310018539429), ('Persian_Farsi', 0.5617388486862183), ('Python_programming', 0.5586454272270203)]\n",
      "['model', 'task', 'use', 'natural', 'train', 'learn', 'pre', 'propose', 'base']\n",
      "[('natural', 0.8305986523628235), ('computer', 0.8039324283599854), ('speak', 0.7833366394042969), ('translate', 0.7695532441139221), ('nlp', 0.7674071788787842), ('informal', 0.7588580250740051), ('nlu', 0.758335530757904), ('markup', 0.7546813488006592), ('formulaic', 0.7322909832000732), ('declarative', 0.7302923202514648)]\n"
     ]
    }
   ],
   "source": [
    "WORD=\"language\"\n",
    "print(g300_vectors.most_similar(WORD, topn=15))\n",
    "print(get_nearest_neighbor(word=WORD,ppmi_matrix=ppmi_2020,k=10))\n",
    "print(trained_cs_vectors.wv.most_similar(WORD, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('info', 0.7363681793212891), ('infomation', 0.680029571056366), ('infor_mation', 0.673384964466095), ('informaiton', 0.6639009118080139), ('informa_tion', 0.6601256728172302), ('informationon', 0.6339334845542908), ('informationabout', 0.6320980191230774), ('Information', 0.6186580657958984), ('informaion', 0.6093292236328125), ('details', 0.6063088774681091), ('inforamtion', 0.5957241654396057), ('Clorox_visit_www.TheCloroxCompany.com', 0.5866811275482178), ('please_visit_http://www.nyx.com', 0.5864027738571167), ('visit_http://www.sybase.com', 0.5852304100990295), ('visit_www.overlandstorage.com', 0.5848026871681213)]\n",
      "['data', 'model', 'propose', 'use', 'paper', 'approach', 'task', 'methods', 'knowledge']\n",
      "[('sensitive', 0.7092013955116272), ('data', 0.7004376649856567), ('geographic', 0.6983487010002136), ('content', 0.689644455909729), ('personal', 0.6846344470977783), ('context', 0.6784451007843018), ('contained', 0.676192045211792), ('metadata', 0.6727553009986877), ('relevant', 0.6669832468032837), ('location', 0.6533821821212769)]\n"
     ]
    }
   ],
   "source": [
    "WORD=\"information\"\n",
    "print(g300_vectors.most_similar(WORD, topn=15))\n",
    "print(get_nearest_neighbor(word=WORD,ppmi_matrix=ppmi_2020,k=10))\n",
    "print(trained_cs_vectors.wv.most_similar(WORD, topn=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('boc1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c60fe3f79824bd88507c0d8875bd9a3c6ed4b495af41f3c0b2e6206d3df018a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "from data_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_mapping(year:str):\n",
    "    '''\n",
    "    Build index to word and word to index map for given year\n",
    "    If year is not available, build map for entire corpus\n",
    "    '''\n",
    "    target_files = [x for x in os.listdir(data_path)]\n",
    "    index2word={}\n",
    "    word2index={}\n",
    "    if year == \"all\":\n",
    "        for f in target_files:\n",
    "            temp = open(data_path+f,\"r\").read()\n",
    "            for word in temp.split():\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "                    index2word[len(word2index)-1] = word\n",
    "    else:\n",
    "        temp = open(data_path+f\"{year}.txt\",\"r\").read()\n",
    "        for word in temp.split():\n",
    "            if word not in word2index:\n",
    "                word2index[word] = len(word2index)\n",
    "                index2word[len(word2index)-1] = word\n",
    "\n",
    "    return index2word, word2index\n",
    "\n",
    "def Get_Unigram(word:str, year:str):\n",
    "    '''\n",
    "    return frequency of a word in text from a given year\n",
    "    '''\n",
    "    f = open(data_path+f\"{year}.txt\",\"r\").read()\n",
    "    unigram = Counter(ngrams(f.split(),1))\n",
    "    return unigram[(word,)] if (word,) in unigram else \"Not Exist\"\n",
    "\n",
    "def Get_topk(k:int, year:str):\n",
    "    '''\n",
    "    Get top k common words in the text from given year\n",
    "    '''\n",
    "    f = open(data_path+f\"{year}.txt\",\"r\").read()\n",
    "    freq = FreqDist(f.split())\n",
    "    return freq.most_common(k)\n",
    "\n",
    "def Get_sentences(year:str):\n",
    "    '''\n",
    "    get sentences from unprocessed data for a given year\n",
    "    '''\n",
    "    data = json.load(open(data_path+'unprocessed.json'))\n",
    "    abstract = \"\"\n",
    "    for record in data[year]:\n",
    "        abstract += record[\"abstract\"]\n",
    "    sentences = sent_tokenize(abstract)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].replace(\"\\n\",\" \")\n",
    "        temp = remove_punctuation(sentences[i])\n",
    "        temp = to_lower_case(temp)\n",
    "        temp = remove_stopwords(temp.split())\n",
    "        temp = lemmatise_verbs(temp)\n",
    "        temp = remove_numbers(temp)\n",
    "        sentences[i] = \" \".join(temp)\n",
    "        \n",
    "    for s in sentences:    \n",
    "        if (not s) or (len(s.split())<2):\n",
    "            sentences.remove(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2794\n"
     ]
    }
   ],
   "source": [
    "index2word, word2index = Build_mapping(year=\"1994\")\n",
    "print(len(word2index))  #unique tokens in each year, or all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precise': 0,\n",
       " 'formulation': 1,\n",
       " 'derivation': 2,\n",
       " 'tree': 3,\n",
       " 'adjoin': 4,\n",
       " 'grammars': 5,\n",
       " 'important': 6,\n",
       " 'ramifications': 7,\n",
       " 'wide': 8,\n",
       " 'variety': 9,\n",
       " 'use': 10,\n",
       " 'formalism': 11,\n",
       " 'syntactic': 12,\n",
       " 'analysis': 13,\n",
       " 'semantic': 14,\n",
       " 'interpretation': 15,\n",
       " 'statistical': 16,\n",
       " 'language': 17,\n",
       " 'model': 18,\n",
       " 'argue': 19,\n",
       " 'definition': 20,\n",
       " 'must': 21,\n",
       " 'reformulate': 22,\n",
       " 'order': 23,\n",
       " 'manifest': 24,\n",
       " 'proper': 25,\n",
       " 'linguistic': 26,\n",
       " 'dependencies': 27,\n",
       " 'derivations': 28,\n",
       " 'particular': 29,\n",
       " 'proposal': 30,\n",
       " 'precisely': 31,\n",
       " 'characterizable': 32,\n",
       " 'tag': 33,\n",
       " 'equivalence': 34,\n",
       " 'class': 35,\n",
       " 'computationally': 36,\n",
       " 'operational': 37,\n",
       " 'virtue': 38,\n",
       " 'compilation': 39,\n",
       " 'linear': 40,\n",
       " 'index': 41,\n",
       " 'together': 42,\n",
       " 'efficient': 43,\n",
       " 'algorithm': 44,\n",
       " 'recognition': 45,\n",
       " 'parse': 46,\n",
       " 'accord': 47,\n",
       " 'compile': 48,\n",
       " 'grammar': 49,\n",
       " 'report': 50,\n",
       " 'recent': 51,\n",
       " 'loebner': 52,\n",
       " 'prize': 53,\n",
       " 'competition': 54,\n",
       " 'inspire': 55,\n",
       " 'turing': 56,\n",
       " 'test': 57,\n",
       " 'intelligent': 58,\n",
       " 'behavior': 59,\n",
       " 'presentation': 60,\n",
       " 'cover': 61,\n",
       " 'structure': 62,\n",
       " 'outcome': 63,\n",
       " 'first': 64,\n",
       " 'instantiation': 65,\n",
       " 'actual': 66,\n",
       " 'event': 67,\n",
       " 'purpose': 68,\n",
       " 'design': 69,\n",
       " 'appropriateness': 70,\n",
       " 'clear': 71,\n",
       " 'prevent': 72,\n",
       " 'useful': 73,\n",
       " 'inappropriate': 74,\n",
       " 'give': 75,\n",
       " 'current': 76,\n",
       " 'level': 77,\n",
       " 'technology': 78,\n",
       " 'speculate': 79,\n",
       " 'suitable': 80,\n",
       " 'alternatives': 81,\n",
       " 'synchronous': 82,\n",
       " 'variant': 83,\n",
       " 'standard': 84,\n",
       " 'intend': 85,\n",
       " 'allow': 86,\n",
       " 'transduction': 87,\n",
       " 'addition': 88,\n",
       " 'specification': 89,\n",
       " 'previous': 90,\n",
       " 'work': 91,\n",
       " 'relation': 92,\n",
       " 'define': 93,\n",
       " 'appeal': 94,\n",
       " 'iterative': 95,\n",
       " 'rewrite': 96,\n",
       " 'process': 97,\n",
       " 'problematic': 98,\n",
       " 'greatly': 99,\n",
       " 'extend': 100,\n",
       " 'expressivity': 101,\n",
       " 'make': 102,\n",
       " 'algorithms': 103,\n",
       " 'difficult': 104,\n",
       " 'impossible': 105,\n",
       " 'introduce': 106,\n",
       " 'simple': 107,\n",
       " 'natural': 108,\n",
       " 'base': 109,\n",
       " 'isomorphisms': 110,\n",
       " 'avoid': 111,\n",
       " 'implementability': 112,\n",
       " 'problems': 113,\n",
       " 'original': 114,\n",
       " 'decrease': 115,\n",
       " 'would': 116,\n",
       " 'otherwise': 117,\n",
       " 'method': 118,\n",
       " 'unusable': 119,\n",
       " 'offset': 120,\n",
       " 'incorporation': 121,\n",
       " 'alternative': 122,\n",
       " 'previously': 123,\n",
       " 'propose': 124,\n",
       " 'completely': 125,\n",
       " 'separate': 126,\n",
       " 'reason': 127,\n",
       " 'thereby': 128,\n",
       " 'practical': 129,\n",
       " 'entertain': 130,\n",
       " 'nonetheless': 131,\n",
       " 'remain': 132,\n",
       " 'case': 133,\n",
       " 'call': 134,\n",
       " 'yet': 135,\n",
       " 'flexibility': 136,\n",
       " 'isomorphism': 137,\n",
       " 'requirement': 138,\n",
       " 'may': 139,\n",
       " 'relax': 140,\n",
       " 'future': 141,\n",
       " 'research': 142,\n",
       " 'tune': 143,\n",
       " 'exact': 144,\n",
       " 'requirements': 145,\n",
       " 'allowable': 146,\n",
       " 'mappings': 147,\n",
       " 'dependency': 148,\n",
       " 'usually': 149,\n",
       " 'interpret': 150,\n",
       " 'equivalent': 151,\n",
       " 'strict': 152,\n",
       " 'form': 153,\n",
       " 'x': 154,\n",
       " 'bar': 155,\n",
       " 'theory': 156,\n",
       " 'forbid': 157,\n",
       " 'stack': 158,\n",
       " 'nod': 159,\n",
       " 'eg': 160,\n",
       " 'n': 161,\n",
       " 'immediately': 162,\n",
       " 'dominate': 163,\n",
       " 'head': 164,\n",
       " 'adequate': 165,\n",
       " 'account': 166,\n",
       " 'one': 167,\n",
       " 'anaphora': 168,\n",
       " 'semantics': 169,\n",
       " 'multiple': 170,\n",
       " 'modifiers': 171,\n",
       " 'require': 172,\n",
       " 'accordingly': 173,\n",
       " 'salvage': 174,\n",
       " 'reinterpret': 175,\n",
       " 'claim': 176,\n",
       " 'phrase': 177,\n",
       " 'map': 178,\n",
       " 'onto': 179,\n",
       " 'binary': 180,\n",
       " 'branch': 181,\n",
       " 'rather': 182,\n",
       " 'flat': 183,\n",
       " 'ones': 184,\n",
       " 'paper': 185,\n",
       " 'show': 186,\n",
       " 'apply': 187,\n",
       " 'memoization': 188,\n",
       " 'cache': 189,\n",
       " 'subgoals': 190,\n",
       " 'associate': 191,\n",
       " 'answer': 192,\n",
       " 'substitutions': 193,\n",
       " 'constraint': 194,\n",
       " 'logic': 195,\n",
       " 'program': 196,\n",
       " 'set': 197,\n",
       " 'motivate': 198,\n",
       " 'desire': 199,\n",
       " 'clp': 200,\n",
       " 'involve': 201,\n",
       " 'interleave': 202,\n",
       " 'coroutining': 203,\n",
       " 'gb': 204,\n",
       " 'hpsg': 205,\n",
       " 's92': 206,\n",
       " 'begin': 207,\n",
       " 'thousand,': 208,\n",
       " 'nine': 209,\n",
       " 'hundred': 210,\n",
       " 'and': 211,\n",
       " 'eighty-seven': 212,\n",
       " 'analyze': 213,\n",
       " 'word': 214,\n",
       " 'frequencies': 215,\n",
       " 'present': 216,\n",
       " 'day': 217,\n",
       " 'spanish': 218,\n",
       " 'speech': 219,\n",
       " 'pathology': 220,\n",
       " 'evaluation': 221,\n",
       " 'tool': 222,\n",
       " 'five': 223,\n",
       " 'two': 224,\n",
       " 'thousand': 225,\n",
       " 'sample': 226,\n",
       " 'children': 227,\n",
       " 'adolescents': 228,\n",
       " 'adults': 229,\n",
       " 'input': 230,\n",
       " 'eighty-eight': 231,\n",
       " 'ninety-one': 232,\n",
       " 'calculations': 233,\n",
       " 'do': 234,\n",
       " 'ninety-two': 235,\n",
       " 'lewandowski': 236,\n",
       " 'analyse': 237,\n",
       " 'carry': 238,\n",
       " 'ninety-three': 239,\n",
       " 'generalization': 240,\n",
       " 'categorial': 241,\n",
       " 'lexical': 242,\n",
       " 'categories': 243,\n",
       " 'mean': 244,\n",
       " 'recursive': 245,\n",
       " 'constraints': 246,\n",
       " 'introduction': 247,\n",
       " 'relational': 248,\n",
       " 'capture': 249,\n",
       " 'effect': 250,\n",
       " 'rule': 251,\n",
       " 'attractive': 252,\n",
       " 'manner': 253,\n",
       " 'illustrate': 254,\n",
       " 'merit': 255,\n",
       " 'new': 256,\n",
       " 'approach': 257,\n",
       " 'syntax': 258,\n",
       " 'dutch': 259,\n",
       " 'cross': 260,\n",
       " 'serial': 261,\n",
       " 'position': 262,\n",
       " 'scope': 263,\n",
       " 'adjuncts': 264,\n",
       " 'constructions': 265,\n",
       " 'delay': 266,\n",
       " 'contain': 267,\n",
       " 'system': 268,\n",
       " 'generate': 269,\n",
       " 'parsers': 270,\n",
       " 'directly': 271,\n",
       " 'metaphor': 272,\n",
       " 'deduction': 273,\n",
       " 'represent': 274,\n",
       " 'systems': 275,\n",
       " 'single': 276,\n",
       " 'engine': 277,\n",
       " 'implement': 278,\n",
       " 'correspond': 279,\n",
       " 'parser': 280,\n",
       " 'generalize': 281,\n",
       " 'easily': 282,\n",
       " 'augment': 283,\n",
       " 'formalisms': 284,\n",
       " 'definite': 285,\n",
       " 'clause': 286,\n",
       " 'rapid': 287,\n",
       " 'prototyping': 288,\n",
       " 'include': 289,\n",
       " 'variants': 290,\n",
       " 'lexicalize': 291,\n",
       " 'context': 292,\n",
       " 'free': 293,\n",
       " 'relationship': 294,\n",
       " 'functional': 295,\n",
       " 'lfg': 296,\n",
       " 'f': 297,\n",
       " 'sentence': 298,\n",
       " 'interpretations': 299,\n",
       " 'express': 300,\n",
       " 'fragment': 301,\n",
       " 'way': 302,\n",
       " 'explain': 303,\n",
       " 'correctly': 304,\n",
       " 'constrain': 305,\n",
       " 'interactions': 306,\n",
       " 'quantifier': 307,\n",
       " 'ambiguity': 308,\n",
       " 'bind': 309,\n",
       " 'deductive': 310,\n",
       " 'framework': 311,\n",
       " 'compositional': 312,\n",
       " 'properties': 313,\n",
       " 'quantify': 314,\n",
       " 'expressions': 315,\n",
       " 'obviate': 316,\n",
       " 'need': 317,\n",
       " 'additional': 318,\n",
       " 'mechanisms': 319,\n",
       " 'cooper': 320,\n",
       " 'storage': 321,\n",
       " 'different': 322,\n",
       " 'scopes': 323,\n",
       " 'might': 324,\n",
       " 'take': 325,\n",
       " 'instead': 326,\n",
       " 'contribution': 327,\n",
       " 'record': 328,\n",
       " 'ordinary': 329,\n",
       " 'logical': 330,\n",
       " 'formula': 331,\n",
       " 'whose': 332,\n",
       " 'proof': 333,\n",
       " 'establish': 334,\n",
       " 'ensure': 335,\n",
       " 'scoped': 336,\n",
       " 'exactly': 337,\n",
       " 'see': 338,\n",
       " 'recast': 339,\n",
       " 'pereira': 340,\n",
       " 'higher': 341,\n",
       " 'intuitionistic': 342,\n",
       " 'provide': 343,\n",
       " 'much': 344,\n",
       " 'direct': 345,\n",
       " 'flexible': 346,\n",
       " 'mechanism': 347,\n",
       " 'least': 348,\n",
       " 'range': 349,\n",
       " 'phenomena': 350,\n",
       " 'develop': 351,\n",
       " 'preliminary': 352,\n",
       " 'prolog': 353,\n",
       " 'implementation': 354,\n",
       " 'deductions': 355,\n",
       " 'describe': 356,\n",
       " 'intensional': 357,\n",
       " 'verbs': 358,\n",
       " 'seek': 359,\n",
       " 'object': 360,\n",
       " 'either': 361,\n",
       " 'individual': 362,\n",
       " 'type': 363,\n",
       " 'produce': 364,\n",
       " 'de': 365,\n",
       " 'dicto': 366,\n",
       " 'read': 367,\n",
       " 'without': 368,\n",
       " 'stipulate': 369,\n",
       " 'raise': 370,\n",
       " 'follow': 371,\n",
       " 'resemble': 372,\n",
       " 'ways': 373,\n",
       " 'differ': 374,\n",
       " 'greater': 375,\n",
       " 'maintain': 376,\n",
       " 'connection': 377,\n",
       " 'result': 378,\n",
       " 'able': 379,\n",
       " 'certain': 380,\n",
       " 'complex': 381,\n",
       " 'derivable': 382,\n",
       " 'purely': 383,\n",
       " 'interface': 384,\n",
       " 'part': 385,\n",
       " 'ongoing': 386,\n",
       " 'within': 387,\n",
       " 'germanic': 388,\n",
       " 'verb': 389,\n",
       " 'cluster': 390,\n",
       " 'observe': 391,\n",
       " 'narrow': 392,\n",
       " 'extension': 393,\n",
       " 'systematic': 394,\n",
       " 'treat': 395,\n",
       " 'members': 396,\n",
       " 'subcat': 397,\n",
       " 'list': 398,\n",
       " 'powerful': 399,\n",
       " 'techniques': 400,\n",
       " 'many': 401,\n",
       " 'applications': 402,\n",
       " 'necessary': 403,\n",
       " 'determine': 404,\n",
       " 'likelihood': 405,\n",
       " 'combination': 406,\n",
       " 'example': 407,\n",
       " 'recognizer': 408,\n",
       " 'combinations': 409,\n",
       " 'eat': 410,\n",
       " 'peach': 411,\n",
       " 'beach': 412,\n",
       " 'likely': 413,\n",
       " 'nlp': 414,\n",
       " 'methods': 415,\n",
       " 'frequency': 416,\n",
       " 'train': 417,\n",
       " 'corpus': 418,\n",
       " 'however': 419,\n",
       " 'nature': 420,\n",
       " 'infrequent': 421,\n",
       " 'occur': 422,\n",
       " 'estimate': 423,\n",
       " 'probability': 424,\n",
       " 'unseen': 425,\n",
       " 'available': 426,\n",
       " 'information': 427,\n",
       " 'similar': 428,\n",
       " 'probabilistic': 429,\n",
       " 'association': 430,\n",
       " 'distributional': 431,\n",
       " 'similarity': 432,\n",
       " 'improve': 433,\n",
       " 'bigrams': 434,\n",
       " 'katz': 435,\n",
       " 'back': 436,\n",
       " 'yield': 437,\n",
       " 'twenty': 438,\n",
       " 'perplexity': 439,\n",
       " 'improvement': 440,\n",
       " 'prediction': 441,\n",
       " 'statistically': 442,\n",
       " 'significant': 443,\n",
       " 'reductions': 444,\n",
       " 'error': 445,\n",
       " 'temporal': 446,\n",
       " 'relations': 447,\n",
       " 'hold': 448,\n",
       " 'events': 449,\n",
       " 'successive': 450,\n",
       " 'utterances': 451,\n",
       " 'often': 452,\n",
       " 'leave': 453,\n",
       " 'implicit': 454,\n",
       " 'underspecified': 455,\n",
       " 'address': 456,\n",
       " 'role': 457,\n",
       " 'respect': 458,\n",
       " 'recovery': 459,\n",
       " 'referential': 460,\n",
       " 'tense': 461,\n",
       " 'impose': 462,\n",
       " 'coherence': 463,\n",
       " 'several': 464,\n",
       " 'facets': 465,\n",
       " 'identification': 466,\n",
       " 'integration': 467,\n",
       " 'complete': 468,\n",
       " 'collection': 469,\n",
       " 'reference': 470,\n",
       " 'interest': 471,\n",
       " 'intonational': 472,\n",
       " 'miscellaneous': 473,\n",
       " 'intonation': 474,\n",
       " 'welcome': 475,\n",
       " 'send': 476,\n",
       " 'juliaresearchattcom': 477,\n",
       " 'drive': 478,\n",
       " 'generation': 479,\n",
       " 'run': 480,\n",
       " 'none': 481,\n",
       " 'daughter': 482,\n",
       " 'constituents': 483,\n",
       " 'syntacto': 484,\n",
       " 'schemata': 485,\n",
       " 'fit': 486,\n",
       " 'shieber': 487,\n",
       " 'et': 488,\n",
       " 'al': 489,\n",
       " 'ninety': 490,\n",
       " 'representations': 491,\n",
       " 'discourse': 492,\n",
       " 'representation': 493,\n",
       " 'udrss': 494,\n",
       " 'frank': 495,\n",
       " 'reyle': 496,\n",
       " 'since': 497,\n",
       " 'general': 498,\n",
       " 'simply': 499,\n",
       " 'return': 500,\n",
       " 'demonstrate': 501,\n",
       " 'feasibility': 502,\n",
       " 'generality': 503,\n",
       " 'basis': 504,\n",
       " 'logically': 505,\n",
       " 'well': 506,\n",
       " 'treatment': 507,\n",
       " 'movement': 508,\n",
       " 'ad': 509,\n",
       " 'hoc': 510,\n",
       " 'solutions': 511,\n",
       " 'exist': 512,\n",
       " 'far': 513,\n",
       " 'unrestricted': 514,\n",
       " 'text': 515,\n",
       " 'score': 516,\n",
       " 'function': 517,\n",
       " 'select': 518,\n",
       " 'best': 519,\n",
       " 'pearl': 520,\n",
       " 'time': 521,\n",
       " 'asynchronous': 522,\n",
       " 'bottom': 523,\n",
       " 'chart': 524,\n",
       " 'earley': 525,\n",
       " 'top': 526,\n",
       " 'pursue': 527,\n",
       " 'highest': 528,\n",
       " 'extent': 529,\n",
       " 'predict': 530,\n",
       " 'attempt': 531,\n",
       " 'stochastic': 532,\n",
       " 'richer': 533,\n",
       " 'conditional': 534,\n",
       " 'probabilities': 535,\n",
       " 'also': 536,\n",
       " 'incorporate': 537,\n",
       " 'assignment': 538,\n",
       " 'unknown': 539,\n",
       " 'feature': 540,\n",
       " 'traditional': 541,\n",
       " 'pipeline': 542,\n",
       " 'architecture': 543,\n",
       " 'successful': 544,\n",
       " 'resolve': 545,\n",
       " 'correct': 546,\n",
       " 'loosely': 547,\n",
       " 'picky': 548,\n",
       " 'agenda': 549,\n",
       " 'technique': 550,\n",
       " 'lead': 551,\n",
       " 'acceptable': 552,\n",
       " 'suboptimal': 553,\n",
       " 'search': 554,\n",
       " 'significantly': 555,\n",
       " 'reduce': 556,\n",
       " 'number': 557,\n",
       " 'edge': 558,\n",
       " 'cky': 559,\n",
       " 'like': 560,\n",
       " 'robustness': 561,\n",
       " 'pure': 562,\n",
       " 'accuracy': 563,\n",
       " 'experiment': 564,\n",
       " 'impact': 565,\n",
       " 'upon': 566,\n",
       " 'efficiency': 567,\n",
       " 'generative': 568,\n",
       " 'hbg': 569,\n",
       " 'advantage': 570,\n",
       " 'detail': 571,\n",
       " 'structural': 572,\n",
       " 'disambiguation': 573,\n",
       " 'novel': 574,\n",
       " 'bracket': 575,\n",
       " 'treebank': 576,\n",
       " 'decision': 577,\n",
       " 'build': 578,\n",
       " 'tease': 579,\n",
       " 'relevant': 580,\n",
       " 'aspects': 581,\n",
       " 'stand': 582,\n",
       " 'contrast': 583,\n",
       " 'usual': 584,\n",
       " 'tailor': 585,\n",
       " 'via': 586,\n",
       " 'introspection': 587,\n",
       " 'hope': 588,\n",
       " 'robust': 589,\n",
       " 'p': 590,\n",
       " 'cfg': 591,\n",
       " 'outperform': 592,\n",
       " 'increase': 593,\n",
       " 'rate': 594,\n",
       " 'sixty': 595,\n",
       " 'seventy-five': 596,\n",
       " 'thirty-seven': 597,\n",
       " 'reduction': 598,\n",
       " 'finite': 599,\n",
       " 'state': 600,\n",
       " 'segment': 601,\n",
       " 'chinese': 602,\n",
       " 'dictionary': 603,\n",
       " 'entries': 604,\n",
       " 'productively': 605,\n",
       " 'derive': 606,\n",
       " 'pronunciations': 607,\n",
       " 'personal': 608,\n",
       " 'name': 609,\n",
       " 'evaluate': 610,\n",
       " 'performance': 611,\n",
       " 'fact': 612,\n",
       " 'people': 613,\n",
       " 'agree': 614,\n",
       " 'segmentation': 615,\n",
       " 'arduous': 616,\n",
       " 'consume': 617,\n",
       " 'grammarians': 618,\n",
       " 'majority': 619,\n",
       " 'grammarian': 620,\n",
       " 'efforts': 621,\n",
       " 'devote': 622,\n",
       " 'hypothesize': 623,\n",
       " 'dictate': 624,\n",
       " 'constituent': 625,\n",
       " 'relationships': 626,\n",
       " 'among': 627,\n",
       " 'ambiguous': 628,\n",
       " 'exceptions': 629,\n",
       " 'corrections': 630,\n",
       " 'automatic': 631,\n",
       " 'acquire': 632,\n",
       " 'initial': 633,\n",
       " 'pitfalls': 634,\n",
       " 'seemingly': 635,\n",
       " 'endless': 636,\n",
       " 'development': 637,\n",
       " 'distributionally': 638,\n",
       " 'linguistically': 639,\n",
       " 'assign': 640,\n",
       " 'distribution': 641,\n",
       " 'space': 642,\n",
       " 'amount': 643,\n",
       " 'contextual': 644,\n",
       " 'potentially': 645,\n",
       " 'highly': 646,\n",
       " 'accurate': 647,\n",
       " 'criteria': 648,\n",
       " 'selection': 649,\n",
       " 'entropy': 650,\n",
       " 'human': 651,\n",
       " 'intuition': 652,\n",
       " 'consider': 653,\n",
       " 'ten': 654,\n",
       " 'year': 655,\n",
       " 'period': 656,\n",
       " 'material': 657,\n",
       " 'measure': 658,\n",
       " 'try': 659,\n",
       " 'maximize': 660,\n",
       " 'achieve': 661,\n",
       " 'seventy-eight': 662,\n",
       " 'compare': 663,\n",
       " 'sixty-nine': 664,\n",
       " 'facts': 665,\n",
       " 'concern': 666,\n",
       " 'ellipsis': 667,\n",
       " 'interclausal': 668,\n",
       " 'question': 669,\n",
       " 'whether': 670,\n",
       " 'behind': 671,\n",
       " 'empty': 672,\n",
       " 'anaphoric': 673,\n",
       " 'interact': 674,\n",
       " 'inference': 675,\n",
       " 'namely': 676,\n",
       " 'common': 677,\n",
       " 'topic': 678,\n",
       " 'coherent': 679,\n",
       " 'situation': 680,\n",
       " 'utilize': 681,\n",
       " 'plan': 682,\n",
       " 'response': 683,\n",
       " 'collaborative': 684,\n",
       " 'consultation': 685,\n",
       " 'dialogues': 686,\n",
       " 'emphasis': 687,\n",
       " 'consultant': 688,\n",
       " 'user': 689,\n",
       " 'execute': 690,\n",
       " 'agent': 691,\n",
       " 'disagree': 692,\n",
       " 'contribute': 693,\n",
       " 'overall': 694,\n",
       " 'problem': 695,\n",
       " 'solve': 696,\n",
       " 'modify': 697,\n",
       " 'cycle': 698,\n",
       " 'collaboration': 699,\n",
       " 'initiate': 700,\n",
       " 'subdialogues': 701,\n",
       " 'negotiate': 702,\n",
       " 'additions': 703,\n",
       " 'share': 704,\n",
       " 'support': 705,\n",
       " 'handle': 706,\n",
       " 'unify': 707,\n",
       " 'negotiation': 708,\n",
       " 'domain': 709,\n",
       " 'action': 710,\n",
       " 'beliefs': 711,\n",
       " 'furthermore': 712,\n",
       " 'cooperative': 713,\n",
       " 'responses': 714,\n",
       " 'sometimes': 715,\n",
       " 'never': 716,\n",
       " 'degrade': 717,\n",
       " 'task': 718,\n",
       " 'noisy': 719,\n",
       " 'image': 720,\n",
       " 'candidates': 721,\n",
       " 'high': 722,\n",
       " 'knowledge': 723,\n",
       " 'source': 724,\n",
       " 'candidate': 725,\n",
       " 'visual': 726,\n",
       " 'inter': 727,\n",
       " 'facilitate': 728,\n",
       " 'link': 729,\n",
       " 'inside': 730,\n",
       " 'page': 731,\n",
       " 'systematically': 732,\n",
       " 'conversation': 733,\n",
       " 'person': 734,\n",
       " 'refer': 735,\n",
       " 'know': 736,\n",
       " 'participant': 737,\n",
       " 'agents': 738,\n",
       " 'collaborate': 739,\n",
       " 'sort': 740,\n",
       " 'salient': 741,\n",
       " 'attribute': 742,\n",
       " 'referent': 743,\n",
       " 'understand': 744,\n",
       " 'confidence': 745,\n",
       " 'adequacy': 746,\n",
       " 'identify': 747,\n",
       " 'judgment': 748,\n",
       " 'suggestion': 749,\n",
       " 'elaboration': 750,\n",
       " 'move': 751,\n",
       " 'refashion': 752,\n",
       " 'inadequate': 753,\n",
       " 'expression': 754,\n",
       " 'cue': 755,\n",
       " 'sense': 756,\n",
       " 'explicitly': 757,\n",
       " 'signal': 758,\n",
       " 'sentential': 759,\n",
       " 'convey': 760,\n",
       " 'explore': 761,\n",
       " 'machine': 762,\n",
       " 'learn': 763,\n",
       " 'classify': 764,\n",
       " 'cgrendel': 765,\n",
       " 'c45': 766,\n",
       " 'induce': 767,\n",
       " 'classification': 768,\n",
       " 'pre': 769,\n",
       " 'effective': 770,\n",
       " 'automate': 771,\n",
       " 'span': 772,\n",
       " 'widely': 773,\n",
       " 'assume': 774,\n",
       " 'units': 775,\n",
       " 'segmental': 776,\n",
       " 'weak': 777,\n",
       " 'consensus': 778,\n",
       " 'recognize': 779,\n",
       " 'quantitative': 780,\n",
       " 'study': 781,\n",
       " 'spontaneous': 782,\n",
       " 'narrative': 783,\n",
       " 'monologues': 784,\n",
       " 'reliability': 785,\n",
       " 'speaker': 786,\n",
       " 'intention': 787,\n",
       " 'criterion': 788,\n",
       " 'subject': 789,\n",
       " 'segmentations': 790,\n",
       " 'correlation': 791,\n",
       " 'three': 792,\n",
       " 'noun': 793,\n",
       " 'pause': 794,\n",
       " 'retrieval': 795,\n",
       " 'metrics': 796,\n",
       " 'compute': 797,\n",
       " 'gram': 798,\n",
       " 'procedure': 799,\n",
       " 'alleviate': 800,\n",
       " 'grams': 801,\n",
       " 'estimation': 802,\n",
       " 'sparse': 803,\n",
       " 'data': 804,\n",
       " 'lack': 805,\n",
       " 'others': 806,\n",
       " 'operate': 807,\n",
       " 'computation': 808,\n",
       " 'substring': 809,\n",
       " 'expectations': 810,\n",
       " 'turn': 811,\n",
       " 'accomplish': 812,\n",
       " 'equations': 813,\n",
       " 'discuss': 814,\n",
       " 'experience': 815,\n",
       " 'hide': 816,\n",
       " 'markov': 817,\n",
       " 'merge': 818,\n",
       " 'strategy': 819,\n",
       " 'omohundro': 820,\n",
       " 'maximum': 821,\n",
       " 'hmm': 822,\n",
       " 'encode': 823,\n",
       " 'successively': 824,\n",
       " 'bayesian': 825,\n",
       " 'posterior': 826,\n",
       " 'stop': 827,\n",
       " 'heuristic': 828,\n",
       " 'possible': 829,\n",
       " 'priors': 830,\n",
       " 'hmms': 831,\n",
       " 'approximations': 832,\n",
       " 'computational': 833,\n",
       " 'baum': 834,\n",
       " 'welch': 835,\n",
       " 'languages': 836,\n",
       " 'small': 837,\n",
       " 'positive': 838,\n",
       " 'find': 839,\n",
       " 'particularly': 840,\n",
       " 'second': 841,\n",
       " 'application': 842,\n",
       " 'label': 843,\n",
       " 'timit': 844,\n",
       " 'database': 845,\n",
       " 'compact': 846,\n",
       " 'pronunciation': 847,\n",
       " 'finally': 848,\n",
       " 'combine': 849,\n",
       " 'neural': 850,\n",
       " 'network': 851,\n",
       " 'acoustic': 852,\n",
       " 'estimators': 853,\n",
       " 'lexicology': 854,\n",
       " 'competence': 855,\n",
       " 'orient': 856,\n",
       " 'abstract': 857,\n",
       " 'away': 858,\n",
       " 'specific': 859,\n",
       " 'domains': 860,\n",
       " 'severe': 861,\n",
       " 'complexity': 862,\n",
       " 'acquisition': 863,\n",
       " 'reusability': 864,\n",
       " 'bottleneck': 865,\n",
       " 'memory': 866,\n",
       " 'consequences': 867,\n",
       " 'phonology': 868,\n",
       " 'morphology': 869,\n",
       " 'translate': 870,\n",
       " 'japanese': 871,\n",
       " 'nouns': 872,\n",
       " 'english': 873,\n",
       " 'face': 874,\n",
       " 'article': 875,\n",
       " 'composition': 876,\n",
       " 'property': 877,\n",
       " 'respectively': 878,\n",
       " 'fairly': 879,\n",
       " 'reliably': 880,\n",
       " 'write': 881,\n",
       " 'expert': 882,\n",
       " 'obtain': 883,\n",
       " 'eight': 884,\n",
       " 'fifty-five': 885,\n",
       " 'construction': 886,\n",
       " 'texts': 887,\n",
       " 'six': 888,\n",
       " 'eighty-nine': 889,\n",
       " 'fifty-six': 890,\n",
       " 'decidable': 891,\n",
       " 'strongly': 892,\n",
       " 'cfgs': 893,\n",
       " 'cubic': 894,\n",
       " 'parsable': 895,\n",
       " 'serve': 896,\n",
       " 'lcfgs': 897,\n",
       " 'schabes': 898,\n",
       " 'water': 899,\n",
       " 'considerably': 900,\n",
       " 'less': 901,\n",
       " 'restriction': 902,\n",
       " 'normal': 903,\n",
       " 'local': 904,\n",
       " 'regular': 905,\n",
       " 'employ': 906,\n",
       " 'intensive': 907,\n",
       " 'elements': 908,\n",
       " 'communicative': 909,\n",
       " 'appropriate': 910,\n",
       " 'grammatical': 911,\n",
       " 'instructional': 912,\n",
       " 'ig': 913,\n",
       " 'precondition': 914,\n",
       " 'explanation': 915,\n",
       " 'extract': 916,\n",
       " 'specialize': 917,\n",
       " 'faster': 918,\n",
       " 'lower': 919,\n",
       " 'price': 920,\n",
       " 'loss': 921,\n",
       " 'coverage': 922,\n",
       " 'specify': 923,\n",
       " 'cut': 924,\n",
       " 'operationality': 925,\n",
       " 'manually': 926,\n",
       " 'automatically': 927,\n",
       " 'value': 928,\n",
       " 'node': 929,\n",
       " 'sufficiently': 930,\n",
       " 'glr': 931,\n",
       " 'recently': 932,\n",
       " 'version': 933,\n",
       " 'lr': 934,\n",
       " 'almost': 935,\n",
       " 'ignore': 936,\n",
       " 'unrecognizable': 937,\n",
       " 'maximal': 938,\n",
       " 'close': 939,\n",
       " 'subsets': 940,\n",
       " 'integrate': 941,\n",
       " 'scheme': 942,\n",
       " 'deem': 943,\n",
       " 'conduct': 944,\n",
       " 'thesis': 945,\n",
       " 'formalization': 946,\n",
       " 'interfacing': 947,\n",
       " 'existence': 948,\n",
       " 'capable': 949,\n",
       " 'mediate': 950,\n",
       " 'surface': 951,\n",
       " 'string': 952,\n",
       " 'literal': 953,\n",
       " 'focus': 954,\n",
       " 'relate': 955,\n",
       " 'term': 956,\n",
       " 'primitives': 957,\n",
       " 'meaningful': 958,\n",
       " 'underlie': 959,\n",
       " 'functionalities': 960,\n",
       " 'instance': 961,\n",
       " 'formal': 962,\n",
       " 'abductive': 963,\n",
       " 'equivalential': 964,\n",
       " 'translation': 965,\n",
       " 'aet': 966,\n",
       " 'respond': 967,\n",
       " 'command': 968,\n",
       " 'completeness': 969,\n",
       " 'meta': 970,\n",
       " 'assertions': 971,\n",
       " 'ldt': 972,\n",
       " 'gamma': 973,\n",
       " 'goal': 974,\n",
       " 'construct': 975,\n",
       " 'permit': 976,\n",
       " 'assumptions': 977,\n",
       " 'formulas': 978,\n",
       " 'equivalences': 979,\n",
       " 'horn': 980,\n",
       " 'clauses': 981,\n",
       " 'description': 982,\n",
       " 'sketch': 983,\n",
       " 'realization': 984,\n",
       " 'literature': 985,\n",
       " 'elegant': 986,\n",
       " 'solution': 987,\n",
       " 'doctor': 988,\n",
       " 'board': 989,\n",
       " 'relativization': 990,\n",
       " 'world': 991,\n",
       " 'assumption': 992,\n",
       " 'ideas': 993,\n",
       " 'concretely': 994,\n",
       " 'sri': 995,\n",
       " 'clare': 996,\n",
       " 'project': 997,\n",
       " 'real': 998,\n",
       " 'payments': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 53), ('model', 51), ('base', 44), ('language', 42), ('paper', 38)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Get_topk(k=5,year=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Get_Unigram(word=\"use\",year=\"1994\")  # get frequency of a word in a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=Get_sentences(\"1999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['schedule dialogs people negotiate time appointments common everyday life',\n",
       " 'paper report result depth empirical investigation resolve explicit temporal reference schedule dialogs',\n",
       " 'four phase work data annotation evaluation model development system implementation evaluation model evaluation analysis',\n",
       " 'system model develop primarily one set data apply later much complex data set assess generalizability model task perform',\n",
       " 'many different type empirical methods apply pinpoint strengths weaknesses approach',\n",
       " 'detail annotation instructions develop intercoder reliability study perform show naive annotators reliably perform target annotations',\n",
       " 'fully automatic system develop evaluate unseen test data good result data set',\n",
       " 'adopt pure realization recency base focus model identify precisely adequate task address',\n",
       " 'addition system result depth evaluation model present base detail manual annotations',\n",
       " 'result errors occur specifically due model focus use set anaphoric relations define model low ambiguity data setstreebanks penn treebank ptb offer simple approach obtain broad coverage grammar one simply read grammar parse tree treebank',\n",
       " 'grammar easy obtain square root rate growth rule set corpus size suggest derive grammar far complete much treebanked text would require obtain complete grammar one exist limit',\n",
       " 'however offer alternative explanation term underspecification structure within treebank',\n",
       " 'hypothesis explore apply algorithm compact derive grammar eliminate redundant rule rule whose right hand side parse rule',\n",
       " 'size result compact grammar significantly less full treebank grammar show approach limit',\n",
       " 'however compact grammar yield good performance figure',\n",
       " 'version compaction algorithm take rule probabilities account propose argue linguistically motivate',\n",
       " 'combine simple thresholding method use give 58 reduction grammar size without significant change parse performance produce 69 reduction gain recall loss precisionthe paper argue fodor lepore misguide attack pustejovsky generative lexicon largely argument rest traditional implausible discredit view lexicon effectively empty content view stand long line explain word mean ostension b explain mean vacuous symbol lexicon often word typographic transmogrification',\n",
       " 'b share wrong belief word must correspond simple entity mean',\n",
       " 'turn semantic rule pustejovsky use argue first although novel feature well establish artificial intelligence tradition explain mean reference structure mention structure assign word may occur close proximity first',\n",
       " 'argue fodor lepore view cannot rule without foundation indeed systems use rule prove practical worth computational systems',\n",
       " 'justification descend line argument whose high point probably wittgenstein quine mean understand simple link world ostensive otherwise relationship whole cultural representational structure world wholethis paper compare task part speech pos tag word sense tag disambiguation wsd argue task relate fineness grain anything like quite different kinds task particularly becuase nothing pos correspond sense novelty',\n",
       " 'paper also argue reintegration sub task separate evaluationlinguistic annotation cover descriptive analytic notations apply raw language data',\n",
       " 'basic data may form time function audio video physiological record may textual',\n",
       " 'add notations may include transcriptions sort phonetic feature discourse structure part speech sense tag syntactic analysis name entity identification co reference annotation',\n",
       " 'several ongoing efforts provide format tool annotations publish annotate linguistic databases lack widely accept standards become critical problem',\n",
       " 'propose standards extent exist focus file format',\n",
       " 'paper focus instead logical structure linguistic annotations',\n",
       " 'survey wide variety exist annotation format demonstrate common conceptual core annotation graph',\n",
       " 'provide formal framework construct maintain search linguistic annotations remain consistent many alternative data structure file formatsrecent technological advance make possible build real time interactive speak dialogue systems wide variety applications',\n",
       " 'however users respect limitations systems performance typically degrade',\n",
       " 'although users differ respect knowledge system limitations although different dialogue strategies make system limitations apparent users current systems try improve performance adapt dialogue behavior individual users',\n",
       " 'paper present empirical evaluation toot adaptable speak dialogue system retrieve train schedule web',\n",
       " 'conduct experiment 20 users carry 4 task adaptable non adaptable versions toot result corpus 80 dialogues',\n",
       " 'value wide range evaluation measure extract corpus',\n",
       " 'result show adaptable toot generally outperform non adaptable toot utility adaptation depend toot initial dialogue strategiescontext sensitive rewrite rule widely use several areas natural language process include syntax morphology phonology speech process',\n",
       " 'kaplan kay karttunen mohri sproat give various algorithms compile rewrite rule finite state transducers',\n",
       " 'present paper extend work allow limit form backreferencing rule',\n",
       " 'explicit use backreferencing lead elegant general solutionsthe two principal areas natural language process research pragmatics belief model speech act process',\n",
       " 'belief model development techniques represent mental attitudes dialogue participant',\n",
       " 'latter approach speech act process base speech act theory involve view dialogue plan term',\n",
       " 'utterances dialogue model step plan understand utterance involve derive complete plan speaker attempt achieve',\n",
       " 'however previous speech act base approach limit reliance upon relatively simplistic belief model techniques relationship plan plan recognition',\n",
       " 'particular techniques assume precomputed nest belief structure',\n",
       " 'paper present approach speech act process base novel belief model techniques nest beliefs propagate demandthis paper link prosody information text process speaker',\n",
       " 'describe operation output loq text speech implementation include model limit attention work memory',\n",
       " 'attentional limitations key',\n",
       " 'vary attentional parameter simulations vary turn count give new text therefore intonational contour utter',\n",
       " 'currently system produce prosody three different style child like adult expressive knowledgeable',\n",
       " 'prosody also exhibit differences within style two simulations alike',\n",
       " 'limit resource approach capture stylistic individual variety find natural prosodycorpus base grammar induction generally rely hand parse train data learn structure language',\n",
       " 'unfortunately cost build large annotate corpora prohibitively expensive',\n",
       " 'work aim improve induction strategy label train data',\n",
       " 'show informative linguistic constituents higher nod parse tree typically denote complex noun phrase sentential clauses',\n",
       " 'account 20 constituents',\n",
       " 'induce grammars sparsely label train data eg higher level constituent label propose adaptation strategy produce grammars parse almost well grammars induce fully label corpora',\n",
       " 'result suggest partial parser replace human annotators must able automatically extract higher level constituents rather base noun phrasesparticles fullfill several distinct central roles japanese language',\n",
       " 'mark arguments well adjuncts functional semantic funtions',\n",
       " 'however straightforward match particles function eg ga mark subject object adjunct sentence',\n",
       " 'particles cooccur',\n",
       " 'verbal arguments could identify particles eliminate japanese sentence',\n",
       " 'finally speak language particles often omit',\n",
       " 'proper treatment particles thus necessary make analysis japanese sentence possible',\n",
       " 'treatment base empirical investigation 800 dialogues',\n",
       " 'set type hierarchy particles motivate subcategorizational modificational behaviour',\n",
       " 'type hierarchy part japanese syntax verbmobilthis paper present new approach partial parse context free structure',\n",
       " 'approach base markov model',\n",
       " 'layer result structure represent markov model output lower layer pass input next higher layer',\n",
       " 'empirical evaluation method yield good result np pp chunk german newspaper textsthe nwo priority programme language speech technology 5 year research programme aim development speak language information systems',\n",
       " 'programme two alternative natural language process nlp modules develop parallel grammar base conventional rule base module data orient memory base stochastic dop module',\n",
       " 'order compare nlp modules formal evaluation carry three years start programme',\n",
       " 'paper describe evaluation procedure evaluation result',\n",
       " 'grammar base component perform much better data orient one comparisongrammatical relationships important level natural language process',\n",
       " 'present trainable approach find relationships transformation sequence error drive learn',\n",
       " 'approach find grammatical relationships core syntax group bypass much parse phase',\n",
       " 'train test set procedure achieve 636 recall 773 precision f score 698previous work context natural language query temporal databases establish method map automatically large subset english time relate question suitable expressions temporal logic like language call top',\n",
       " 'algorithm translate top tsql2 temporal database language also define',\n",
       " 'paper show top expressions could translate simpler logic like language call bot',\n",
       " 'bot close traditional first order predicate logic fopl hence exist methods manipulate fopl expressions exploit interface time sensitive applications tsql2 databases maintain exist english top mappingthis paper explore automatic construction multilingual lexical knowledge base pre exist lexical resources',\n",
       " 'present new robust approach link already exist lexical semantic hierarchies',\n",
       " 'use constraint satisfaction algorithm relaxation label select among candidate translations propose bilingual dictionary right english wordnet synset sense taxonomy automatically derive spanish monolingual dictionary',\n",
       " 'although average 15 possible wordnet connections sense taxonomy method achieve accuracy 80',\n",
       " 'finally also propose several ways technique could apply enrich improve exist lexical databaseswe argue grammatical analysis viable alternative concept spot process speak input practical speak dialogue system',\n",
       " 'discuss structure grammar model robust parse combine linguistic source information statistical source information',\n",
       " 'discuss test result suggest grammatical process allow fast accurate process speak inputwe present approach machine translation combine ideas methodologies example base lexicalist theoretical frameworks',\n",
       " 'approach implement multilingual machine translation systemin recent work present formal framework linguistic annotation base label acyclic digraphs',\n",
       " 'annotation graph offer simple yet powerful method represent complex annotation structure incorporate hierarchy overlap',\n",
       " 'motivate illustrate approach use discourse level annotations text speech data draw callhome coconut muc 7 damsl train annotation scheme',\n",
       " 'help domain specialists construct hybrid multi level annotation fragment boston university radio speech corpus include follow level segment word breath tobi tilt treebank coreference name entity',\n",
       " 'show annotation graph represent hybrid multi level structure derive diverse set file format',\n",
       " 'also show approach facilitate substantive comparison multiple annotations single signal base different theoretical model',\n",
       " 'discussion show annotation graph open door wide range integration tool format corporadividing sentence chunk word useful preprocessing step parse information extraction information retrieval',\n",
       " 'ramshaw marcus 1995 introduce convenient data representation chunk convert tag task',\n",
       " 'paper examine seven different data representations problem recognize noun phrase chunk',\n",
       " 'show data representation choice minor influence chunk performance',\n",
       " 'however equip suitable data representation memory base learn chunker able improve best publish chunk result standard data setthis paper propose japanese english cross language information retrieval clir system target technical document',\n",
       " 'system first translate give query contain technical term target language retrieve document relevant translate query',\n",
       " 'translation technical term still problematic technical term often compound word thus new term progressively create simply combine exist base word',\n",
       " 'addition japanese often represent loanwords base phonogram',\n",
       " 'consequently exist dictionaries find difficult achieve sufficient coverage',\n",
       " 'counter first problem use compound word translation method use bilingual dictionary base word collocational statistics resolve translation ambiguity',\n",
       " 'second problem propose transliteration method identify phonetic equivalents target language',\n",
       " 'also show effectiveness system use test collection clirin paper present application explanation base learn ebl parse module real time english spanish machine translation system design translate close caption',\n",
       " 'discuss efficiency coverage trade off available ebl introduce techniques use increase coverage maintain high level space time efficiency',\n",
       " 'performance result indicate approach effectivea statistical classification algorithm application language identification noisy input describe',\n",
       " 'main innovation compute confidence limit classification algorithm terminate enough evidence make clear decision make avoid problems categories similar characteristics',\n",
       " 'second application genre identification briefly examine',\n",
       " 'result show problems language identification techniques avoid illustrate important point statistical language process use provide feedback success ratewe propose parser constraint logic grammars implement hpsg combine advantage dynamic bottom advance top control',\n",
       " 'parser allow user apply magic compilation specific constraints grammar result process dynamically bottom goal direct fashion',\n",
       " 'state art top process techniques use deal remain constraints',\n",
       " 'discuss various aspects concern implementation parser part grammar development systemwe describe recently develop corpus annotation scheme evaluate parsers avoid shortcomings current methods',\n",
       " 'scheme encode grammatical relations head dependents use mark new public domain corpus naturally occur english text',\n",
       " 'show corpus use evaluate accuracy robust parser relate corpus extant resourceswe describe method automatically generate lexical transfer rule ltrs word equivalences use transfer rule templates',\n",
       " 'templates skeletal ltrs unspecified word',\n",
       " 'new ltrs create instantiate template word provide word belong appropriate lexical categories require template',\n",
       " 'define two methods create inventory templates use generate new ltrs',\n",
       " 'simpler method consist extract finite set templates sample hand cod ltrs directly use generation process',\n",
       " 'method consist abstract initial finite set templates define higher level templates bilingual equivalences define term correspondences involve phrasal categories',\n",
       " 'phrasal templates map onto set lexical templates aid grammars',\n",
       " 'way infinite set lexical templates recursively define',\n",
       " 'new ltrs create parse input word match template phrasal level use correspond lexical categories instantiate lexical template',\n",
       " 'definition infinite set templates enable automatic creation ltrs multi word non compositional word equivalences cardinalitythe paper describe speech speech translation system intarc develop first phase verbmobil project',\n",
       " 'general design goals intarc system architecture time synchronous process well incrementality interactivity mean achieve higher degree robustness scalability',\n",
       " 'interactivity mean addition bottom term process level data flow ability process top restrictions consider signal segment process level',\n",
       " 'construction intarc 20 operational since fall 1996 follow engineer approach focus integration symbolic linguistic stochastic recognition techniques lead generalization concept one pass beam searchthis paper address novel task detect sub topic correspondence pair text fragment enhance common notions text similarity',\n",
       " 'task address couple correspond term subsets bipartite cluster',\n",
       " 'paper present cost base cluster scheme compare bipartite version single link method provide illustrate resultsthis paper describe robust parse techniques fruitful apply build query generation module part pipelined nlp architecture aim process natural language query restrict domain',\n",
       " 'want show semantic robustness represent key issue nlp systems likely partial ill form utterances due various factor eg',\n",
       " 'noisy environments low quality speech recognition modules etc necessary succeed even partially extract meaningful informationthis paper propose efficient example sample method example base word sense disambiguation systems',\n",
       " 'construct database practical size considerable overhead manual sense disambiguation overhead supervision require',\n",
       " 'addition time complexity search large size database pose considerable problem overhead search',\n",
       " 'counter problems method selectively sample smaller size effective subset give example set use word sense disambiguation',\n",
       " 'method characterize reliance notion train utility degree example informative future example sample use train system',\n",
       " 'system progressively collect examples select greatest utility',\n",
       " 'paper report effectiveness method experiment one thousand sentence',\n",
       " 'compare experiment example sample methods method reduce overhead supervision overhead search without degeneration performance systemseveral methods discuss construct finite automaton give context free grammar include methods lead subsets lead supersets original context free language',\n",
       " 'methods regular approximation new others present refine form respect exist literature',\n",
       " 'practical experiment different methods regular approximation perform speak language input hypotheses speech recognizer filter finite automatonquestion answer task do trec8 use english document',\n",
       " 'examine question answer task japanese sentence',\n",
       " 'method select answer match question sentence knowledge base data write natural language',\n",
       " 'use syntactic information obtain highly accurate answersrecent developments theoretical linguistics lead widespread acceptance constraint base analyse prosodic morphology phenomena truncation infixation float morphemes reduplication',\n",
       " 'reduplication particularly challenge state art computational morphology since involve copy part phonological string',\n",
       " 'paper argue certain extensions one level model phonology morphology bird ellison 1994 cover computational aspects prosodic morphology use finite state methods',\n",
       " 'nutshell enrich lexical representations provide additional automaton arc repeat skip sound also allow insertion additional material',\n",
       " 'kind resource consciousness introduce control additional freedom distinguish producer consumer arc',\n",
       " 'non finite state copy aspect reduplication map automata intersection non finite state operation',\n",
       " 'bound local optimization prune certain automaton arc fail contribute linguistic optimisation criteria',\n",
       " 'paper present implement case study ulwa construct state infixation german hypocoristic truncation tagalog apply reduplication illustrate expressive power approach merit limitations discuss possible extensions sketch',\n",
       " 'conclude one level approach prosodic morphology present attractive way extend finite state techniques difficult phenomena hitherto resist elegant computational analysesa noun phrase indirectly refer entity already mention',\n",
       " 'example go old house last night',\n",
       " 'roof leak badly indicate roof associate old house mention previous sentence',\n",
       " 'kind reference indirect anaphora study well natural language process important coherence resolution language understand machine translation',\n",
       " 'order analyze indirect anaphora need case frame dictionary nouns contain knowledge relationships two nouns dictionary presently exist',\n",
       " 'therefore force use examples x x verb case frame dictionary instead',\n",
       " 'try estimate indirect anaphora use information obtain recall rate 63 precision rate 68 test sentence',\n",
       " 'indicate information x useful certain extent cannot make use noun case frame dictionary',\n",
       " 'estimate result would give noun case frame dictionary obtain recall precision rat 71 82 respectively',\n",
       " 'finally propose way construct noun case frame dictionary use examples x',\n",
       " 'paper present method estimate referents demonstrative pronouns personal pronouns zero pronouns japanese sentence use examples surface expressions topics foci',\n",
       " 'unlike conventional work semantic markers semantic constraints use examples semantic constraints show experiment examples useful semantic markers',\n",
       " 'also propose many new methods estimate referents pronouns',\n",
       " 'example use form x estimate referents demonstrative adjectives',\n",
       " 'addition new methods use many conventional methods',\n",
       " 'result experiment use methods obtain precision rate 87 estimate referents demonstrative pronouns personal pronouns zero pronouns train sentence obtain precision rate 78 test sentencesin machine translation man machine dialogue important clarify referents noun phrase',\n",
       " 'present method determine referents noun phrase japanese sentence use referential properties modifiers possessors noun phrase',\n",
       " 'since japanese language article difficult decide whether noun phrase antecedent',\n",
       " 'previously estimate referential properties noun phrase correspond article use clue word sentence',\n",
       " 'use referential properties system determine referents noun phrase japanese sentence',\n",
       " 'furthermore use modifiers possessors noun phrase determine referents noun phrase',\n",
       " 'result train sentence obtain precision rate 82 recall rate 85 determination referents noun phrase antecedents',\n",
       " 'test sentence obtain precision rate 79 recall rate 77verbs sometimes omit japanese sentence',\n",
       " 'necessary recover omit verbs purpose language understand machine translation conversational process',\n",
       " 'paper describe practical way recover omit verbs use surface expressions examples',\n",
       " 'experiment resolution verb ellipses use information obtain recall rate 73 precision rate 66 test sentenceswe develop new method japanese english translation tense aspect modality use example base method',\n",
       " 'method similarity input example sentence define degree semantic match expressions end sentence',\n",
       " 'method also use k nearest neighbor method order exclude effect noise example wrongly tag data bilingual corpora',\n",
       " 'experiment show method translate tense aspects modalities accurately top level mt software currently available market',\n",
       " 'moreover require hand craft rulesa system describe use mix level representation part mean natural language document base standard horn clause logic variable depth search strategy distinguish different level abstraction knowledge representation locate specific passages document',\n",
       " 'mix level representations well variable depth search strategies applicable field outside nlpa system describe use mix level knowledge representation base standard horn clause logic represent part mean natural language document',\n",
       " 'variable depth search strategy outline distinguish different level abstraction knowledge representation locate specific passages document',\n",
       " 'detail description linguistic aspects system give',\n",
       " 'mix level representations well variable depth search strategies applicable field outside nlpthe amount electronic document internet grow quickly',\n",
       " 'effectively identify subject document become important issue',\n",
       " 'past research focus behavior nouns document',\n",
       " 'although subject compose nouns constituents determine nouns subject nouns',\n",
       " 'base assumption texts well organize event drive nouns verbs together contribute process subject identification',\n",
       " 'paper consider four factor 1 word importance 2 word frequency 3 word co occurrence 4 word distance propose model identify subject textual document',\n",
       " 'preliminary experiment show performance propose model close human beingsthis paper follow dymetman1998 present approach grammar description process base geometry cancellation diagram concept play central role combinatorial group theory lyndon schuppe1977',\n",
       " 'focus geometric intuitions relate group theoretical diagram traditional chart associate context free grammars type 0 rewrite systems',\n",
       " 'paper structure follow',\n",
       " 'begin section 1 analyze chart term construct call cells geometrical counterpart rule',\n",
       " 'move section 2 presentation cancellation diagram show use computationally',\n",
       " 'section 3 give formal algebraic presentation concept group computation structure base standard notions free group conjugacy',\n",
       " 'relate section 4 geometric algebraic view computation use fundamental theorem combinatorial group theory rotman1994',\n",
       " 'section 5 study detail relationship two view basis simple grammar state group computation structure',\n",
       " 'section 6 extend grammar handle non local construct relative pronouns quantifiers',\n",
       " 'conclude section 7 brief note differences normal submonoids normal subgroups group computation versus rewrite systems use group morphisms study computational complexity parse generationmixed metaphors neglect recent metaphor research',\n",
       " 'paper suggest neglect short sight',\n",
       " 'though mix complex phenomenon straight metaphors kinds reason knowledge structure require',\n",
       " 'paper provide analysis parallel serial mix metaphors within framework ai system already capable reason straight metaphorical manifestations argue process underlie mix central metaphorical mean',\n",
       " 'therefore theory metaphors must able account mixingthis paper present model base unsupervised algorithm recover word boundaries natural language text delete',\n",
       " 'algorithm derive probability model source generate text',\n",
       " 'fundamental structure model specify abstractly detail component model phonology word order word frequency replace modular fashion',\n",
       " 'model yield language independent prior probability distribution possible sequence possible word give alphabet base assumption input generate concatenate word fix unknown lexicon',\n",
       " 'model unusual treat generation complete corpus regardless length single event probability space',\n",
       " 'accordingly algorithm estimate probability distribution word instead attempt calculate prior probabilities various word sequence could underlie observe text',\n",
       " 'experiment phonemic transcripts spontaneous speech parent young children suggest algorithm effective propose algorithms least utterance boundaries give text include substantial number short utterances',\n",
       " 'keywords bayesian grammar induction probability model minimum description length mdl unsupervised learn cognitive model language acquisition segmentationthe paper describe extensive experiment inside outside estimation lexicalize probabilistic context free grammar german verb final clauses',\n",
       " 'grammar formalism feature make experiment feasible describe',\n",
       " 'successive model evaluate precision recall phrase markupwe present probabilistic model constraint base grammars method estimate parameters model incomplete ie unparsed data',\n",
       " 'whereas methods exist estimate parameters probabilistic context free grammars incomplete data baum 1970 far probabilistic grammars involve context dependencies parameter estimation techniques complete ie fully parse data present abney 1997',\n",
       " 'however complete data estimation require labor intensive error prone grammar specific hand annotate large language corpora',\n",
       " 'present log linear probability model constraint logic program general algorithm estimate parameters model incomplete data extend estimation algorithm della pietra della pietra lafferty 1997 incomplete data settingsin paper discuss cascade memory base grammatical relations assignment',\n",
       " 'first stag cascade find chunk several type npvpadjpadvppp label adverbial function eg',\n",
       " 'local temporal',\n",
       " 'last stage assign grammatical relations pair chunk',\n",
       " 'study effect add several level cascade classifier find even less perform chunkers enhance performance relation finderwe present memory base learn mbl approach shallow parse pos tag chunk identification syntactic relations formulate memory base modules',\n",
       " 'experiment report paper show competitive result f value wall street journal wsj treebank 938 np chunk 947 vp chunk 771 subject detection 790 object detectionthis dissertation analyse computational properties current performance model natural language parse particular data orient parse dop point major shortcomings suggest suitable solutions',\n",
       " 'provide proof various problems probabilistic disambiguation np complete instance performance model argue none model account attractive efficiency properties human language process limit domains eg',\n",
       " 'frequent input usually process faster infrequent ones',\n",
       " 'central hypothesis dissertation shortcomings eliminate specialize performance model limit domains',\n",
       " 'dissertation address grammar model specialization present new framework ambiguity reduction specialization ars framework formulate necessary sufficient condition successful specialization',\n",
       " 'framework instantiate specialization algorithms apply specialize dop',\n",
       " 'novelties learn algorithms 1 limit hypotheses space include safe model 2 express constrain optimization formulae minimize entropy train tree bank give specialize grammar constraint size specialize model exceed predefined maximum 3 enable integrate specialize model original one complementary manner',\n",
       " 'dissertation provide experiment initial implementations compare result specialize dop sdop model original dop model encourage resultsthis article investigate use transformation base error drive learn resolve part speech ambiguity greek language',\n",
       " 'aim study performance also examine dependence different thematic domains',\n",
       " 'result present two different test case corpus management succession events general theme corpus',\n",
       " 'two experiment show performance method depend thematic domain corpus accuracy greek language around 95the article survey little history technology set main current theoretical approach brief discuss go opposition theoretical empirical approach',\n",
       " 'illustrate situation discussion converse system loebner prize 1997 display feature approachesbecause rather fundamental change underlie model propose paper withdraw archivenatural language generation systems nlg map non linguistic representations string word number step use intermediate representations various level abstraction',\n",
       " 'template base systems contrast tend use one representation level ie',\n",
       " 'fix string combine possibly sophisticate way generate final text',\n",
       " 'circumstances may profitable combine nlg template base techniques',\n",
       " 'issue combine generation techniques see abstract term issue mix level representation different degrees linguistic abstraction',\n",
       " 'paper aim define reference architecture systems use mix representations',\n",
       " 'argue mix representations use without abandon linguistically ground approach language generationa statistical model segmentation word discovery child direct speech present',\n",
       " 'incremental unsupervised learn algorithm infer word boundaries base model describe result empirical test show algorithm competitive model use similar task also presentedwe present technique complement hide markov model incorporate lexicalize state represent syntactically uncommon word',\n",
       " 'approach examine distribution transition select uncommon word make lexicalize state word',\n",
       " 'perform part speech tag experiment brown corpus evaluate resultant language model discover technique improve tag accuracy 021 95 level confidencerecently argue autocatalytic theory could apply origin culture',\n",
       " 'possible application theory mean philosophy language call radical interpretation comment upon compare previous applicationswe present technique automatic induction slot annotations subcategorization frame base induction hide class em framework statistical estimation',\n",
       " 'model empirically evalutated general decision test',\n",
       " 'induction slot label subcategorization frame accomplish application em apply experimentally frame observations derive parse large corpora',\n",
       " 'outline interpretation learn representations theoretical linguistic decompositional lexical entriesa pattern base approach presentation codification reuse property specifications finite state verification propose dwyer collegues',\n",
       " 'pattern enable non experts read write formal specifications realistic systems facilitate easy conversion specifications formalisms ltl ctl qre',\n",
       " 'paper extend pattern system events change value variables context ltl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"02g155\".isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['021',\n",
       " '15',\n",
       " '1970',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '20',\n",
       " '58',\n",
       " '63',\n",
       " '636',\n",
       " '66',\n",
       " '68',\n",
       " '69',\n",
       " '698previous',\n",
       " '71',\n",
       " '73',\n",
       " '771',\n",
       " '773',\n",
       " '77verbs',\n",
       " '78',\n",
       " '79',\n",
       " '790',\n",
       " '80',\n",
       " '800',\n",
       " '82',\n",
       " '85',\n",
       " '87',\n",
       " '938',\n",
       " '947',\n",
       " '95',\n",
       " '95the',\n",
       " 'abandon',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abney',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'abstractly',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accomplish',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'achieve',\n",
       " 'acquisition',\n",
       " 'act',\n",
       " 'acyclic',\n",
       " 'adapt',\n",
       " 'adaptable',\n",
       " 'adaptation',\n",
       " 'add',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adequate',\n",
       " 'adjectives',\n",
       " 'adjunct',\n",
       " 'adjuncts',\n",
       " 'adopt',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'adverbial',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aim',\n",
       " 'algebraic',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'alike',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alphabet',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'ambiguity',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'analyse',\n",
       " 'analysesa',\n",
       " 'analysis',\n",
       " 'analytic',\n",
       " 'analyze',\n",
       " 'anaphora',\n",
       " 'anaphoric',\n",
       " 'annotate',\n",
       " 'annotation',\n",
       " 'annotations',\n",
       " 'annotators',\n",
       " 'answer',\n",
       " 'answersrecent',\n",
       " 'antecedent',\n",
       " 'antecedents',\n",
       " 'anything',\n",
       " 'apparent',\n",
       " 'applicable',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applicationswe',\n",
       " 'apply',\n",
       " 'appointments',\n",
       " 'approach',\n",
       " 'approachesbecause',\n",
       " 'appropriate',\n",
       " 'approximation',\n",
       " 'arc',\n",
       " 'architecture',\n",
       " 'archivenatural',\n",
       " 'areas',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'around',\n",
       " 'ars',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artificial',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'assess',\n",
       " 'assign',\n",
       " 'assignment',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumption',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'attentional',\n",
       " 'attitudes',\n",
       " 'attractive',\n",
       " 'audio',\n",
       " 'autocatalytic',\n",
       " 'automata',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'automaton',\n",
       " 'automatonquestion',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'backreferencing',\n",
       " 'badly',\n",
       " 'bank',\n",
       " 'base',\n",
       " 'basic',\n",
       " 'basis',\n",
       " 'baum',\n",
       " 'bayesian',\n",
       " 'beam',\n",
       " 'become',\n",
       " 'becuase',\n",
       " 'begin',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'beingsthis',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'belong',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bilingual',\n",
       " 'bipartite',\n",
       " 'bird',\n",
       " 'boston',\n",
       " 'bot',\n",
       " 'bottom',\n",
       " 'bound',\n",
       " 'boundaries',\n",
       " 'breath',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'broad',\n",
       " 'brown',\n",
       " 'build',\n",
       " 'bypass',\n",
       " 'calculate',\n",
       " 'call',\n",
       " 'callhome',\n",
       " 'cancellation',\n",
       " 'candidate',\n",
       " 'cannot',\n",
       " 'capable',\n",
       " 'caption',\n",
       " 'capture',\n",
       " 'cardinalitythe',\n",
       " 'carry',\n",
       " 'cascade',\n",
       " 'case',\n",
       " 'categories',\n",
       " 'cells',\n",
       " 'central',\n",
       " 'certain',\n",
       " 'challenge',\n",
       " 'change',\n",
       " 'characteristics',\n",
       " 'characterize',\n",
       " 'chart',\n",
       " 'child',\n",
       " 'children',\n",
       " 'choice',\n",
       " 'chunk',\n",
       " 'chunker',\n",
       " 'chunkers',\n",
       " 'circumstances',\n",
       " 'clarify',\n",
       " 'class',\n",
       " 'classification',\n",
       " 'classifier',\n",
       " 'clause',\n",
       " 'clauses',\n",
       " 'clear',\n",
       " 'clir',\n",
       " 'clirin',\n",
       " 'close',\n",
       " 'clue',\n",
       " 'cluster',\n",
       " 'co',\n",
       " 'coconut',\n",
       " 'cod',\n",
       " 'codification',\n",
       " 'cognitive',\n",
       " 'coherence',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'collegues',\n",
       " 'collocational',\n",
       " 'combinatorial',\n",
       " 'combine',\n",
       " 'comment',\n",
       " 'common',\n",
       " 'compact',\n",
       " 'compaction',\n",
       " 'compare',\n",
       " 'comparison',\n",
       " 'comparisongrammatical',\n",
       " 'competitive',\n",
       " 'compilation',\n",
       " 'compile',\n",
       " 'complement',\n",
       " 'complementary',\n",
       " 'complete',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'component',\n",
       " 'compose',\n",
       " 'compositional',\n",
       " 'compound',\n",
       " 'computation',\n",
       " 'computational',\n",
       " 'computationally',\n",
       " 'compute',\n",
       " 'concatenate',\n",
       " 'concept',\n",
       " 'conceptual',\n",
       " 'concern',\n",
       " 'conclude',\n",
       " 'condition',\n",
       " 'conduct',\n",
       " 'confidence',\n",
       " 'confidencerecently',\n",
       " 'conjugacy',\n",
       " 'connections',\n",
       " 'consciousness',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considerable',\n",
       " 'consist',\n",
       " 'consistent',\n",
       " 'constituent',\n",
       " 'constituents',\n",
       " 'constrain',\n",
       " 'constraint',\n",
       " 'constraints',\n",
       " 'construct',\n",
       " 'construction',\n",
       " 'consumer',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'context',\n",
       " 'contour',\n",
       " 'contrast',\n",
       " 'contribute',\n",
       " 'control',\n",
       " 'convenient',\n",
       " 'conventional',\n",
       " 'conversational',\n",
       " 'converse',\n",
       " 'conversion',\n",
       " 'convert',\n",
       " 'cooccur',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'coreference',\n",
       " 'corpora',\n",
       " 'corporadividing',\n",
       " 'corpus',\n",
       " 'correspond',\n",
       " 'correspondence',\n",
       " 'correspondences',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counterpart',\n",
       " 'couple',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'craft',\n",
       " 'create',\n",
       " 'creation',\n",
       " 'criteria',\n",
       " 'critical',\n",
       " 'cross',\n",
       " 'ctl',\n",
       " 'cultural',\n",
       " 'culture',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'damsl',\n",
       " 'data',\n",
       " 'database',\n",
       " 'databases',\n",
       " 'databaseswe',\n",
       " 'deal',\n",
       " 'decide',\n",
       " 'decision',\n",
       " 'decompositional',\n",
       " 'define',\n",
       " 'definition',\n",
       " 'degeneration',\n",
       " 'degrade',\n",
       " 'degree',\n",
       " 'degrees',\n",
       " 'delete',\n",
       " 'della',\n",
       " 'demandthis',\n",
       " 'demonstrate',\n",
       " 'demonstrative',\n",
       " 'denote',\n",
       " 'depend',\n",
       " 'dependence',\n",
       " 'dependencies',\n",
       " 'dependents',\n",
       " 'depth',\n",
       " 'derive',\n",
       " 'descend',\n",
       " 'describe',\n",
       " 'description',\n",
       " 'descriptive',\n",
       " 'design',\n",
       " 'detail',\n",
       " 'detect',\n",
       " 'detection',\n",
       " 'detectionthis',\n",
       " 'determination',\n",
       " 'determine',\n",
       " 'develop',\n",
       " 'development',\n",
       " 'developments',\n",
       " 'diagram',\n",
       " 'dialogs',\n",
       " 'dialogue',\n",
       " 'dialogues',\n",
       " 'dictionaries',\n",
       " 'dictionary',\n",
       " 'differ',\n",
       " 'differences',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digraphs',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'disambiguation',\n",
       " 'discourse',\n",
       " 'discover',\n",
       " 'discovery',\n",
       " 'discredit',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'display',\n",
       " 'dissertation',\n",
       " 'distance',\n",
       " 'distinct',\n",
       " 'distinguish',\n",
       " 'distribution',\n",
       " 'diverse',\n",
       " 'do',\n",
       " 'document',\n",
       " 'domain',\n",
       " 'domains',\n",
       " 'door',\n",
       " 'dop',\n",
       " 'draw',\n",
       " 'drive',\n",
       " 'due',\n",
       " 'dwyer',\n",
       " 'dymetman1998',\n",
       " 'dynamic',\n",
       " 'dynamically',\n",
       " 'easy',\n",
       " 'ebl',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effectivea',\n",
       " 'effectively',\n",
       " 'effectiveness',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'efforts',\n",
       " 'eg',\n",
       " 'electronic',\n",
       " 'elegant',\n",
       " 'eliminate',\n",
       " 'ellipses',\n",
       " 'ellison',\n",
       " 'em',\n",
       " 'empirical',\n",
       " 'empirically',\n",
       " 'empty',\n",
       " 'enable',\n",
       " 'encode',\n",
       " 'encourage',\n",
       " 'end',\n",
       " 'engineer',\n",
       " 'english',\n",
       " 'enhance',\n",
       " 'enough',\n",
       " 'enrich',\n",
       " 'entity',\n",
       " 'entriesa',\n",
       " 'entropy',\n",
       " 'environments',\n",
       " 'equip',\n",
       " 'equivalences',\n",
       " 'equivalents',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'establish',\n",
       " 'estimate',\n",
       " 'estimation',\n",
       " 'etc',\n",
       " 'evaluate',\n",
       " 'evaluation',\n",
       " 'evaluationlinguistic',\n",
       " 'evalutated',\n",
       " 'even',\n",
       " 'event',\n",
       " 'events',\n",
       " 'everyday',\n",
       " 'evidence',\n",
       " 'examine',\n",
       " 'example',\n",
       " 'examples',\n",
       " 'exceed',\n",
       " 'exclude',\n",
       " 'exhibit',\n",
       " 'exist',\n",
       " 'expensive',\n",
       " 'experiment',\n",
       " 'experimentally',\n",
       " 'experts',\n",
       " 'explain',\n",
       " 'explanation',\n",
       " 'explicit',\n",
       " 'exploit',\n",
       " 'explore',\n",
       " 'express',\n",
       " 'expressions',\n",
       " 'expressive',\n",
       " 'extant',\n",
       " 'extend',\n",
       " 'extensions',\n",
       " 'extensive',\n",
       " 'extent',\n",
       " 'extract',\n",
       " 'extraction',\n",
       " 'facilitate',\n",
       " 'factor',\n",
       " 'fail',\n",
       " 'fall',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'feasible',\n",
       " 'feature',\n",
       " 'feedback',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'filter',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finderwe',\n",
       " 'fineness',\n",
       " 'finite',\n",
       " 'first',\n",
       " 'fix',\n",
       " 'float',\n",
       " 'flow',\n",
       " 'foci',\n",
       " 'focus',\n",
       " 'fodor',\n",
       " 'follow',\n",
       " 'fopl',\n",
       " 'force',\n",
       " 'form',\n",
       " 'formal',\n",
       " 'formalism',\n",
       " 'formalisms',\n",
       " 'format',\n",
       " 'formatsrecent',\n",
       " 'formulae',\n",
       " 'formulate',\n",
       " 'foundation',\n",
       " 'four',\n",
       " 'fragment',\n",
       " 'frame',\n",
       " 'framework',\n",
       " 'frameworks',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'frequency',\n",
       " 'frequent',\n",
       " 'fruitful',\n",
       " 'full',\n",
       " 'fullfill',\n",
       " 'fully',\n",
       " 'function',\n",
       " 'functional',\n",
       " 'fundamental',\n",
       " 'funtions',\n",
       " 'furthermore',\n",
       " 'future',\n",
       " 'ga',\n",
       " 'gain',\n",
       " 'general',\n",
       " 'generalizability',\n",
       " 'generalization',\n",
       " 'generally',\n",
       " 'generate',\n",
       " 'generation',\n",
       " 'generationa',\n",
       " 'generationmixed',\n",
       " 'generative',\n",
       " 'genre',\n",
       " 'geometric',\n",
       " 'geometrical',\n",
       " 'geometry',\n",
       " 'german',\n",
       " 'give',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goals',\n",
       " 'good',\n",
       " 'grain',\n",
       " 'grammar',\n",
       " 'grammars',\n",
       " 'grammatical',\n",
       " 'graph',\n",
       " 'greatest',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'head',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'hide',\n",
       " 'hierarchies',\n",
       " 'hierarchy',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highly',\n",
       " 'history',\n",
       " 'hitherto',\n",
       " 'horn',\n",
       " 'house',\n",
       " 'however',\n",
       " 'hpsg',\n",
       " 'human',\n",
       " 'hybrid',\n",
       " 'hypocoristic',\n",
       " 'hypotheses',\n",
       " 'hypothesis',\n",
       " 'ideas',\n",
       " 'identification',\n",
       " 'identify',\n",
       " 'ie',\n",
       " 'ill',\n",
       " 'illustrate',\n",
       " 'implausible',\n",
       " 'implement',\n",
       " 'implementation',\n",
       " 'implementations',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'include',\n",
       " 'incomplete',\n",
       " 'incorporate',\n",
       " 'increase',\n",
       " 'incremental',\n",
       " 'incrementality',\n",
       " 'indeed',\n",
       " 'independent',\n",
       " 'indicate',\n",
       " 'indirect',\n",
       " 'indirectly',\n",
       " 'individual',\n",
       " 'induce',\n",
       " 'induction',\n",
       " 'infer',\n",
       " 'infinite',\n",
       " 'infixation',\n",
       " 'influence',\n",
       " 'information',\n",
       " 'informationthis',\n",
       " 'informative',\n",
       " 'infrequent',\n",
       " 'initial',\n",
       " 'innovation',\n",
       " 'input',\n",
       " 'inputwe',\n",
       " 'insertion',\n",
       " 'inside',\n",
       " 'instance',\n",
       " 'instantiate',\n",
       " 'instead',\n",
       " 'instructions',\n",
       " 'intarc',\n",
       " 'integrate',\n",
       " 'integration',\n",
       " 'intelligence',\n",
       " 'intensive',\n",
       " 'interactive',\n",
       " 'interactivity',\n",
       " 'intercoder',\n",
       " 'interface',\n",
       " 'intermediate',\n",
       " 'internet',\n",
       " 'interpretation',\n",
       " 'intersection',\n",
       " 'intonational',\n",
       " 'introduce',\n",
       " 'intuitions',\n",
       " 'inventory',\n",
       " 'investigate',\n",
       " 'investigation',\n",
       " 'involve',\n",
       " 'issue',\n",
       " 'japanese',\n",
       " 'journal',\n",
       " 'justification',\n",
       " 'kaplan',\n",
       " 'karttunen',\n",
       " 'kay',\n",
       " 'key',\n",
       " 'keywords',\n",
       " 'kind',\n",
       " 'kinds',\n",
       " 'knowledge',\n",
       " 'knowledgeable',\n",
       " 'label',\n",
       " 'labor',\n",
       " 'lack',\n",
       " 'lafferty',\n",
       " 'language',\n",
       " 'large',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'layer',\n",
       " 'lead',\n",
       " 'leak',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'length',\n",
       " 'lepore',\n",
       " 'less',\n",
       " 'level',\n",
       " 'lexical',\n",
       " 'lexicalist',\n",
       " 'lexicalize',\n",
       " 'lexicon',\n",
       " 'life',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limit',\n",
       " 'limitations',\n",
       " 'line',\n",
       " 'linear',\n",
       " 'linguistic',\n",
       " 'linguistically',\n",
       " 'linguistics',\n",
       " 'link',\n",
       " 'literature',\n",
       " 'little',\n",
       " 'loanwords',\n",
       " 'local',\n",
       " 'locate',\n",
       " 'loebner',\n",
       " 'log',\n",
       " 'logic',\n",
       " 'logical',\n",
       " 'long',\n",
       " 'loq',\n",
       " 'loss',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'ltl',\n",
       " 'ltrs',\n",
       " 'lyndon',\n",
       " 'machine',\n",
       " 'magic',\n",
       " 'main',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'make',\n",
       " 'man',\n",
       " 'management',\n",
       " 'manifestations',\n",
       " 'manipulate',\n",
       " 'manner',\n",
       " 'manual',\n",
       " 'many',\n",
       " 'map',\n",
       " 'mappingthis',\n",
       " 'marcus',\n",
       " 'mark',\n",
       " 'markers',\n",
       " 'market',\n",
       " 'markov',\n",
       " 'markupwe',\n",
       " 'match',\n",
       " 'material',\n",
       " 'maximum',\n",
       " 'may',\n",
       " 'mbl',\n",
       " 'mdl',\n",
       " 'mean',\n",
       " 'meaningful',\n",
       " 'measure',\n",
       " 'memory',\n",
       " 'mental',\n",
       " 'mention',\n",
       " 'merit',\n",
       " 'metaphor',\n",
       " 'metaphorical',\n",
       " 'metaphors',\n",
       " 'method',\n",
       " 'methodologies',\n",
       " 'methods',\n",
       " 'minimize',\n",
       " 'minimum',\n",
       " 'minor',\n",
       " 'misguide',\n",
       " 'mix',\n",
       " 'mixingthis',\n",
       " 'modalities',\n",
       " 'modality',\n",
       " 'model',\n",
       " 'modificational',\n",
       " 'modifiers',\n",
       " 'modular',\n",
       " 'module',\n",
       " 'modules',\n",
       " 'mohri',\n",
       " 'monolingual',\n",
       " 'moreover',\n",
       " 'morphemes',\n",
       " 'morphisms',\n",
       " 'morphology',\n",
       " 'motivate',\n",
       " 'move',\n",
       " 'mt',\n",
       " 'muc',\n",
       " 'much',\n",
       " 'multi',\n",
       " 'multilingual',\n",
       " 'multiple',\n",
       " 'must',\n",
       " 'naive',\n",
       " 'name',\n",
       " 'natural',\n",
       " 'naturally',\n",
       " 'nearest',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'neglect',\n",
       " 'negotiate',\n",
       " 'neighbor',\n",
       " 'nest',\n",
       " 'new',\n",
       " 'newspaper',\n",
       " 'next',\n",
       " 'night',\n",
       " 'nlg',\n",
       " 'nlp',\n",
       " 'nlpa',\n",
       " 'nlpthe',\n",
       " 'nod',\n",
       " 'noise',\n",
       " 'noisy',\n",
       " 'non',\n",
       " 'none',\n",
       " 'normal',\n",
       " 'notations',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'notion',\n",
       " 'notions',\n",
       " 'noun',\n",
       " 'nouns',\n",
       " 'novel',\n",
       " 'novelties',\n",
       " 'novelty',\n",
       " 'np',\n",
       " 'npvpadjpadvppp',\n",
       " 'number',\n",
       " 'nutshell',\n",
       " 'nwo',\n",
       " 'object',\n",
       " 'observations',\n",
       " 'observe',\n",
       " 'obtain',\n",
       " 'occur',\n",
       " 'occurrence',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'often',\n",
       " 'old',\n",
       " 'omit',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'ongoing',\n",
       " 'onto',\n",
       " 'open',\n",
       " 'operation',\n",
       " 'operational',\n",
       " 'opposition',\n",
       " 'optimisation',\n",
       " 'optimization',\n",
       " 'order',\n",
       " 'organize',\n",
       " 'orient',\n",
       " 'origin',\n",
       " 'original',\n",
       " 'ostension',\n",
       " 'ostensive',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'outline',\n",
       " 'outperform',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'overhead',\n",
       " 'overlap',\n",
       " 'pair',\n",
       " 'paper',\n",
       " 'parallel',\n",
       " 'parameter',\n",
       " 'parameters',\n",
       " 'parent',\n",
       " 'parse',\n",
       " 'parser',\n",
       " 'parsers',\n",
       " 'part',\n",
       " 'partial',\n",
       " 'partially',\n",
       " 'participant',\n",
       " 'particles',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'pass',\n",
       " 'passages',\n",
       " 'past',\n",
       " 'pattern',\n",
       " 'penn',\n",
       " 'people',\n",
       " 'perform',\n",
       " 'performance',\n",
       " 'personal',\n",
       " 'phase',\n",
       " 'phenomena',\n",
       " 'phenomenon',\n",
       " 'philosophy',\n",
       " 'phonemic',\n",
       " 'phonetic',\n",
       " 'phonogram',\n",
       " 'phonological',\n",
       " 'phonology',\n",
       " 'phrasal',\n",
       " 'phrase',\n",
       " 'phrasesparticles',\n",
       " 'physiological',\n",
       " 'pietra',\n",
       " 'pinpoint',\n",
       " 'pipelined',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'point',\n",
       " 'pos',\n",
       " 'pose',\n",
       " 'possessors',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'power',\n",
       " 'powerful',\n",
       " 'pp',\n",
       " 'practical',\n",
       " 'pragmatics',\n",
       " 'pre',\n",
       " 'precisely',\n",
       " 'precision',\n",
       " 'precisionthe',\n",
       " 'precomputed',\n",
       " 'predefined',\n",
       " 'predicate',\n",
       " 'preliminary',\n",
       " 'preprocessing',\n",
       " 'present',\n",
       " 'presentation',\n",
       " 'presentedwe',\n",
       " 'presently',\n",
       " 'previous',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'principal',\n",
       " 'prior',\n",
       " 'priority',\n",
       " 'prize',\n",
       " 'probabilistic',\n",
       " 'probabilities',\n",
       " 'probability',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problematic',\n",
       " 'problems',\n",
       " 'procedure',\n",
       " 'process',\n",
       " 'produce',\n",
       " 'producer',\n",
       " 'profitable',\n",
       " 'program',\n",
       " 'programme',\n",
       " 'progressively',\n",
       " 'prohibitively',\n",
       " 'project',\n",
       " 'prone',\n",
       " 'pronouns',\n",
       " 'proof',\n",
       " 'propagate',\n",
       " 'proper',\n",
       " 'properties',\n",
       " 'property',\n",
       " 'propose',\n",
       " 'prosodic',\n",
       " 'prosody',\n",
       " 'prosodycorpus',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'proximity',\n",
       " 'prune',\n",
       " 'ptb',\n",
       " 'public',\n",
       " 'publish',\n",
       " 'pure',\n",
       " 'purpose',\n",
       " 'pustejovsky',\n",
       " 'qre',\n",
       " 'quality',\n",
       " 'quantifiers',\n",
       " 'query',\n",
       " 'question',\n",
       " 'quickly',\n",
       " 'quine',\n",
       " 'quite',\n",
       " 'radical',\n",
       " 'radio',\n",
       " 'ramshaw',\n",
       " 'range',\n",
       " 'rat',\n",
       " 'rate',\n",
       " 'ratewe',\n",
       " 'rather',\n",
       " 'raw',\n",
       " 'read',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf_matrix=tf_idf.fit_transform(sentences)\n",
    "tf_idf.get_feature_names()\n",
    "#pd.DataFrame(data = td_idf_matrix.toarray(),columns=vacab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('boc1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c60fe3f79824bd88507c0d8875bd9a3c6ed4b495af41f3c0b2e6206d3df018a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
